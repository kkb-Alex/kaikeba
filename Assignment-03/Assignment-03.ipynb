{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Linear\\ Regresion\\ Model\\ \\Rightarrow y=ax+b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    \"\"\" Using linear regression model to fit dataset\n",
    "    \n",
    "    Fit the dataset on linear regression model provided by sklearn module. \n",
    "    Output the training score, training result, visualize the data and return the predict function\n",
    "    \n",
    "    Args:\n",
    "        X: A set of variable x1, x2, x3, ..., xn\n",
    "        y: A set of dependent variable y1, y2, y3, ..., yn \n",
    "    \n",
    "    Returns:\n",
    "        predict_func: predict function that accept new x and return predicted value y\n",
    "    \"\"\"\n",
    "    X = np.array(X).reshape(-1, 1) # transfer 1*n array to n*1\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # training\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # training result\n",
    "    score = model.score(X, y)\n",
    "    coef = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    # print the result\n",
    "    ic(score)\n",
    "    ic(coef)\n",
    "    ic(intercept)\n",
    "    \n",
    "    # the predict function\n",
    "    def f(x):\n",
    "        return coef * x + intercept\n",
    "    \n",
    "    # draw the line\n",
    "    plt.scatter(X, y)\n",
    "    plt.plot(X, f(X), color='red');\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "import random\n",
    "def assuming_function(x):\n",
    "    # Salary vs cost on foods\n",
    "    return 0.3 * x + 332 + random.randint(-500, 500)\n",
    "X = np.random.randint(800, 20000, size=(100))\n",
    "y = [assuming_function(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n",
      "ic| score: 0.9720980892190736\n",
      "ic| coef: 0.29932958442899604\n",
      "ic| intercept: 273.26928893639933\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "predict_func = linear_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2967.2355487973637"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "predict_func(9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X, y):\n",
    "    \"\"\" Using K-Nearest Neighbor model to fit dataset\n",
    "    \n",
    "    Fit the dataset on KNN model. \n",
    "    Output the predict function\n",
    "    \n",
    "    Args:\n",
    "        X: A set of variable x1, x2, x3, ..., xn\n",
    "        y: A set of dependent variable y1, y2, y3, ..., yn \n",
    "    \n",
    "    Returns:\n",
    "        predict_func: predict function that accept new x and return predicted value y\n",
    "    \"\"\"\n",
    "    dataset = np.array([(Xi, yi) for Xi, yi in zip(X, y)])\n",
    "    \n",
    "    # x is 1 dimension, so using difference as distance\n",
    "    def distance(x1, x2):\n",
    "        return abs(x1 - x2)\n",
    "    \n",
    "    def f(x, k=5):\n",
    "        most_similar = np.array(sorted(dataset, key=lambda xi: distance(xi[0], x))[:k])\n",
    "        predicted_y = sum(y for _, y in most_similar) / len(most_similar) # using average value to predict the y\n",
    "\n",
    "        # draw the graph\n",
    "        plt.scatter(dataset[:, 0], dataset[:, 1])\n",
    "        plt.scatter(most_similar[:, 0], most_similar[:, 1], color='y')\n",
    "        plt.scatter(x, predicted_y, color='red')\n",
    "        \n",
    "        return predicted_y\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2882.5199999999995"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the same data in Linear regression\n",
    "knn_predict = knn_regression(X, y)\n",
    "knn_predict(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2895.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try k=10\n",
    "knn_predict(9000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵：\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p(x_i)log(p(x_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(elements:list):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_spliter(training_data, target: str) -> str:\n",
    "    \"\"\"Sort the features by salience (defined by entropy) and return the best feature in string\n",
    "    Args:\n",
    "        training_data: the training data (xi, yi)\n",
    "        target: the target feature or element of data\n",
    "    Returns:\n",
    "        the best feature of training data\n",
    "    \"\"\"\n",
    "    features = set(training_data.columns.tolist()) - {target}  #\n",
    "    spliters = list()\n",
    "    \n",
    "    for f in features:\n",
    "        values = set(training_data[f])\n",
    "        entropy_v = sum(entropy(training_data[training_data[f] == v][target].tolist()) for v in values)\n",
    "        ic(f)\n",
    "        ic(values)\n",
    "        ic(entropy_v)\n",
    "        spliters.append([f, values, entropy_v])\n",
    "    \n",
    "    spliters.sort(key=lambda x:x[2])\n",
    "    ic(spliters)\n",
    "    return spliters[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mock data in class\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| spliters: [['family_number', {1, 2}, 0.6730116670092565],\n",
      "               ['income', {'-10', '+10'}, 0.6730116670092565],\n",
      "               ['gender', {'F', 'M'}, 1.198849312913621]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'family_number'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_spliter(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| spliters: [['family_number', {1, 2}, 0.5623351446188083],\n",
      "               ['gender', {'F', 'M'}, 0.6365141682948128],\n",
      "               ['income', {'+10'}, 0.6730116670092565]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'family_number'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_spliter(dataset[dataset['income'] == '+10'], 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_for_2d(training_data: pd.DataFrame, n_clusters: int, max_iter=500):\n",
    "    # fit the model\n",
    "    cluster = KMeans(n_clusters=n_clusters, max_iter=max_iter)\n",
    "    cluster.fit(training_data)\n",
    "    \n",
    "    # visualization\n",
    "    plt.figure(1) # graph1\n",
    "    plt.scatter(training_data[:,0], training_data[:,1])\n",
    "    plt.title('Before training')\n",
    "    \n",
    "    plt.figure(2) # gragh2\n",
    "    plt.title('After training')\n",
    "    color = ['black', 'red', 'orange', 'yellow', 'green', 'aqua', \n",
    "             'royalblue', 'darkblue', 'purple', 'pink'] # assume centers are no more than 10\n",
    "    # draw the points after training\n",
    "    for location, i in zip(training_data, cluster.labels_):\n",
    "        plt.scatter(*location, c=color[i])\n",
    "    # draw the centers\n",
    "    for center in cluster.cluster_centers_:\n",
    "        plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testing data\n",
    "X = [random.randint(0, 500) for _ in range(100)]\n",
    "Y = [random.randint(0, 500) for _ in range(100)]\n",
    "dataset = np.array([[x, y] for x, y in zip(X, Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans_for_2d(dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 模型指对问题、现象、客观事物或者规律进行抽象后的表达方式\n",
    "2. 因为模型是抽象之后的产物，只保留了主要的信息或者有用的信息，所以所有的模型都不能完全代表原本的事物；但也正因为模型是抽象后的产物，过滤掉了很多对分析没有帮助的信息，保留了少量有用的信息，所以对分析问题、理解客观事物以及发现规律很有帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 欠拟合 -> 模型未能学习数据中的关系 \n",
    "- 欠拟合的产生原因 -> 对于给定数据集，大多数时候产生欠拟合的原因是模型过于简单，拟合函数能力不够。可以通过增加模型复杂度或者更换其他模型\n",
    "\n",
    "\n",
    "- 过拟合 -> 模型在拟合函数时过分考虑了噪声等不必要的数据关联性，导致模型对训练集拟合十分好而对测试集的预测效果很差\n",
    "- 过拟合的产生原因 -> 模型过于复杂；模型中的系数过大；训练用的数据太少；训练数据分析不均匀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion Matrix(混淆矩阵):\n",
    "<table>\n",
    "    <tr>\n",
    "        <th rowspan=\"2\">Actual</th>\n",
    "        <th colspan=\"2\">Predict</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Positive</td>\n",
    "        <td>Negative</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>True</td>\n",
    "        <td>TP(True Positive)</td>\n",
    "        <td>FN(False Negative)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>False</td>\n",
    "        <td>FP(False Positive)</td>\n",
    "        <td>TN(True Negative)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- <u>Precision(精度)</u> -> 所有预测为True的样本中实际为True的比例\n",
    "$$ Precision=\\frac{TP}{TP+FP} $$\n",
    "当我们希望尽量不把False判断为True时么，Precision越高越好。举个例子，假设去买10个玉石的原石，100个中有20个是真玉石，其他都是普通石头，而买原石的成本很高，那我们希望买到的10个原石全是真玉石，也就是Precision = 1\n",
    "\n",
    "\n",
    "- <u>Recall(召回率)</u> -> 所有实际为True的样本中被预测为True的比例\n",
    "$$ Recall=\\frac{TP}{TP+FN} $$\n",
    "当我们希望把所有实际为True的样本都预测为True时，Recall越高越好。也举个例子，假设雷达侦测到30个可疑信号，其中3个是导弹信号，其他是干扰信号，那我们肯定希望把3个导弹信息全部标记出来，也就是Recall = 1\n",
    "\n",
    "\n",
    "- <u>AUC(Area under Curve of ROC)</u> -> AUS最直接的定义就是ROC曲线下的面积，但是这个定义没有具体的意义。方便理解的定义是：对于一个二分类模型，随机给定一个正样本和一个负样本，分类器输出该正样本为正的概率值比分类器输出该负样本为正的概率值要大的可能性\n",
    "    - 如果要理解AUC，必须先弄懂<u>ROC</u> -> ROC曲线的x轴为FPR(False Positive Rate), y轴为TPR(True Positive Rate), 具体定义为：\n",
    "    $$ FPR=\\frac{FP}{FP+TN} $$\n",
    "    $$ TPR=\\frac{TP}{TP+FN} $$\n",
    "    - 机器学习中的ROC是如何画出来的 -> 混淆矩阵的使用或者准确率(Accuracy)的计算都需要对样本进行分类，但是在实际分析中，很多时候我们计算的结果是样本分类的概率，这样的话，就需要引入一个阈值。但是阈值的选取会很大程度上影响准确率以及混淆矩阵的结果。ROC是在取不同阈值情况下FPR和TPR在2d图中的点相连后的曲线，而曲线下的面积就是AUC。\n",
    "    - 一般情况下，在样本非常不均匀的情况下使用AUC会有比较好的效果，比如在反欺诈场景中，设置欺诈行为是True，但是占比很少(假设0.1%)，那么只要把所有样本判断为False，准确率将高达99.9%，而AUC下将所有样本判断为False，AUC为0.5\n",
    "\n",
    "\n",
    "- <u>F-Score</u> -> 在理想情况下，我们都希望Precision和Recall都高。虽然Precision和Recall在公式上并没有必然的相关性，但是在实际情况下，很多时候这两个指标往往是相互制约的。所以，我们需要对这两个指标进行取舍，根据实际情况分配不同的权重，于是引入F-Score:\n",
    "$$ F\\!-\\!Score=(1+\\beta^2)\\cdot\\frac{Precision\\cdot Recall}{\\beta^2\\cdot Precision+Recall} $$\n",
    "    - 当 $ \\beta=1 $ 时，我们认为Precision和Recall同等重要，此时被称为F1-Score\n",
    "    - 当 $ \\beta>1 $ 时，我们认为Recall更加重要，此时被称为F2-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">&emsp;&emsp;我不是科班出身的，所以对传统的分析式编程的具体含义不是很了解，网上也搜不到相关内容，所以就简单的写一下我接触机器学习这个概念前后我在解决一个问题时思维方式的变化吧。<br>\n",
    "&emsp;&emsp;以预测房价为例，传统的思维方式是，找到和房价相关的变量，然后通过回归等方式拟合出一个函数，并且在对函数本身进行解释的同时对函数的预测能力进行评价解释；在机器学习的思维模式下，虽然同样会进行建模，拟合函数，但是不需要对函数进行解释，也不需要对函数的预测结果进行解释，只需要评价函数的拟合效果和预测能力。<br>\n",
    "&emsp;&emsp;这两者之间最大的差别在于，传统思维模式下，我们认为如果一个函数能很好的预测新数据，那么就需要对我们拟合的函数进行解释，甚至还要和我们的认知进行对比，比如房价和房子的总面积正相关是合理的，但是如果房价和总面积的三次方正相关，那么我们可能会觉得有点难以解释，假设如果房价和销售当月总放晴天数正相关，那我们会觉得这可能存在问题。另外，在传统的思维模式下，我们会主动筛除那些我们认为根本不相关的变量。但是如果使用机器学习，我们不在乎为什么<u>总面积*客厅面积/厨房面积</u>会对房价产生影响，不需要解释为什么模型是这样的，为什么这些变量是有效的，只要拟合模型效果好，有预测能力，那就是一个有用的模型，而且，数据越多，变量越多(在保证拟合效果的前提下)，机器学习的效果就越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">我同意这个观点。对于一个机器学习任务而言，会有很多不同模型的选择，这些模型都能在一定程度上拟合训练数据，但是我们希望能找一个最优的模型，这时一个正确的评价指标就尤为重要。如果评价指标不准确，就会导致选择错误的模型，甚至把整个机器学习的优化导向错误的方向。而且评价模型的指标有很多种，需要根据不同的模型，不同的问题环境，不同的目标进行选择，可以说定义了模型的评价标准就等于是确定了机器学习目标的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure find_best_spliter function\n",
    "def best_spliter(training_data, features, target: str) -> str:\n",
    "    \"\"\"Sort the features by salience (defined by entropy) and return the best feature in string\n",
    "    Args:\n",
    "        training_data: the training data (xi, yi)\n",
    "        features: unused spliters\n",
    "        target: the target feature or element of data\n",
    "    Returns:\n",
    "        the best feature of training data\n",
    "    \"\"\"\n",
    "    spliters = list()\n",
    "    \n",
    "    for f in features:\n",
    "        values = set(training_data[f])\n",
    "        entropy_v = sum(entropy(training_data[training_data[f] == v][target].tolist()) for v in values)\n",
    "        spliters.append([f, values, entropy_v])\n",
    "    \n",
    "    spliters.sort(key=lambda x:x[2])\n",
    "    return spliters[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(training_data: pd.DataFrame, target:str, criterion=entropy):\n",
    "    \"\"\"Fit the data on decision model and return the predict function\n",
    "    Args: \n",
    "        training_data: the data in pd.DataFrame structure\n",
    "        target: the target column's title\n",
    "        criterion: the function to sort the feature. Default is entropy I defined before\n",
    "    Returns:\n",
    "        predict function\n",
    "    \"\"\"\n",
    "    features = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    def get_tree(data, features_unused):\n",
    "        spliter = best_spliter(data, features_unused, target) # get the best spliter\n",
    "        values = set(training_data[spliter])\n",
    "        features_unused -= {spliter}\n",
    "        return [spliter] + [get_details(spliter, value, data,features_unused) for value in values]\n",
    "    \n",
    "    def get_details(spliter, value, data, features_unused):\n",
    "        entropy = criterion(data[data[spliter] == value][target].tolist())\n",
    "        # entropy == 0, return the target value of this group\n",
    "        if entropy == 0:\n",
    "            return [value, entropy, data[data[spliter] == value][target].tolist()[0]]\n",
    "        # entropy != 0, and no more spliters, return the highest probablity value\n",
    "        if not features_unused:\n",
    "            return [value, entropy, majority(data[data[spliter] == value][target].tolist())]\n",
    "        # keep splitting\n",
    "        return [value, entropy, get_tree(data[data[spliter] == value], features_unused)]\n",
    "    \n",
    "    def majority(data):\n",
    "        count = dict()\n",
    "        for v in data:\n",
    "            if v not in count:\n",
    "                count[v] = 1\n",
    "            else:\n",
    "                count[v] += 1\n",
    "        return max(count, key=count.get)\n",
    "    \n",
    "    tree = get_tree(training_data, features)\n",
    "    ic(tree)\n",
    "    \n",
    "    def predict_func(data):\n",
    "        # search the tree until find the target value\n",
    "        def next_node(data, tree):\n",
    "            node = tree[0]\n",
    "            for sub_tree in tree[1:]:\n",
    "                if data[node] == sub_tree[0]:\n",
    "                    if isinstance(sub_tree[2], int):\n",
    "                        return sub_tree[2]\n",
    "                    else:\n",
    "                        return next_node(data, sub_tree[2])\n",
    "        return next_node(data, tree)\n",
    "    return predict_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mock data in class\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tree: ['family_number',\n",
      "           [1,\n",
      "            0.6730116670092565,\n",
      "            ['income',\n",
      "             ['-10', -0.0, 1],\n",
      "             ['+10',\n",
      "              0.5623351446188083,\n",
      "              ['gender', ['F', 0.6931471805599453, 1], ['M', -0.0, 0]]]]],\n",
      "           [2, -0.0, 1]]\n"
     ]
    }
   ],
   "source": [
    "model = decision_tree(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'M', 'income': '-10', 'family_number': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'F', 'income': '+10', 'family_number': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'M', 'income': '+10', 'family_number': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New loss function\n",
    "$$ loss=\\frac{1}{n}\\sum \\left|y_i-\\hat{y_i}\\right|=\\frac{1}{n}\\sum \\sqrt{(y_i-\\hat{y_i})^2}$$\n",
    "$$ loss=\\frac{1}{n}\\sum \\left|y_i-(kx_i+b)\\right|=\\frac{1}{n}\\sum \\sqrt{(y_i-(kx_i+b))^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(y, y_hat)) / len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Derivatives\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}}=\\frac{1}{n}\\sum -x_i\\frac{y_i-\\hat{y_i}}{\\left|y_i-\\hat{y_i}\\right|} $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}}=\\frac{1}{n}\\sum -\\frac{y_i-\\hat{y_i}}{\\left|y_i-\\hat{y_i}\\right|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(x, y, y_hat):\n",
    "        gradient += (-x_i) * (y_i - y_hat_i) / abs(y_i - y_hat_i)\n",
    "    return 1 / n * gradient\n",
    "\n",
    "def partial_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "        gradient -= (y_i - y_hat_i) / abs(y_i - y_hat_i)\n",
    "    return 1 / n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "dataset = load_boston()\n",
    "x, y=dataset['data'], dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target function\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 167.70285108175054, parameters k is -11.226919394373496 and b is -74.61296106773356\n",
      "Iteration 1, the loss is 167.2978847879236, parameters k is -11.164073050499978 and b is -74.60296106773356\n",
      "Iteration 2, the loss is 166.89291849409688, parameters k is -11.10122670662646 and b is -74.59296106773355\n",
      "Iteration 3, the loss is 166.48795220026997, parameters k is -11.03838036275294 and b is -74.58296106773355\n",
      "Iteration 4, the loss is 166.0829859064431, parameters k is -10.975534018879422 and b is -74.57296106773354\n",
      "Iteration 5, the loss is 165.6780196126161, parameters k is -10.912687675005904 and b is -74.56296106773354\n",
      "Iteration 6, the loss is 165.27305331878944, parameters k is -10.849841331132385 and b is -74.55296106773353\n",
      "Iteration 7, the loss is 164.8680870249626, parameters k is -10.786994987258867 and b is -74.54296106773353\n",
      "Iteration 8, the loss is 164.46312073113575, parameters k is -10.724148643385348 and b is -74.53296106773352\n",
      "Iteration 9, the loss is 164.0581544373087, parameters k is -10.66130229951183 and b is -74.52296106773352\n",
      "Iteration 10, the loss is 163.65318814348186, parameters k is -10.598455955638311 and b is -74.51296106773351\n",
      "Iteration 11, the loss is 163.2482218496552, parameters k is -10.535609611764793 and b is -74.50296106773351\n",
      "Iteration 12, the loss is 162.84325555582834, parameters k is -10.472763267891274 and b is -74.4929610677335\n",
      "Iteration 13, the loss is 162.43828926200135, parameters k is -10.409916924017756 and b is -74.4829610677335\n",
      "Iteration 14, the loss is 162.03332296817442, parameters k is -10.347070580144237 and b is -74.4729610677335\n",
      "Iteration 15, the loss is 161.62835667434757, parameters k is -10.284224236270719 and b is -74.46296106773349\n",
      "Iteration 16, the loss is 161.2233903805209, parameters k is -10.2213778923972 and b is -74.45296106773348\n",
      "Iteration 17, the loss is 160.81842408669408, parameters k is -10.158531548523682 and b is -74.44296106773348\n",
      "Iteration 18, the loss is 160.41345779286698, parameters k is -10.095685204650163 and b is -74.43296106773347\n",
      "Iteration 19, the loss is 160.00849149904047, parameters k is -10.032838860776645 and b is -74.42296106773347\n",
      "Iteration 20, the loss is 159.6035252052134, parameters k is -9.969992516903126 and b is -74.41296106773346\n",
      "Iteration 21, the loss is 159.19855891138647, parameters k is -9.907146173029608 and b is -74.40296106773346\n",
      "Iteration 22, the loss is 158.79359261756002, parameters k is -9.844299829156089 and b is -74.39296106773345\n",
      "Iteration 23, the loss is 158.38862632373284, parameters k is -9.78145348528257 and b is -74.38296106773345\n",
      "Iteration 24, the loss is 157.98366002990608, parameters k is -9.718607141409052 and b is -74.37296106773344\n",
      "Iteration 25, the loss is 157.5786937360792, parameters k is -9.655760797535534 and b is -74.36296106773344\n",
      "Iteration 26, the loss is 157.17372744225221, parameters k is -9.592914453662015 and b is -74.35296106773343\n",
      "Iteration 27, the loss is 156.7687611484254, parameters k is -9.530068109788496 and b is -74.34296106773343\n",
      "Iteration 28, the loss is 156.36379485459858, parameters k is -9.467221765914978 and b is -74.33296106773342\n",
      "Iteration 29, the loss is 155.95882856077168, parameters k is -9.40437542204146 and b is -74.32296106773342\n",
      "Iteration 30, the loss is 155.55386226694478, parameters k is -9.341529078167941 and b is -74.31296106773341\n",
      "Iteration 31, the loss is 155.1488959731181, parameters k is -9.278682734294422 and b is -74.3029610677334\n",
      "Iteration 32, the loss is 154.74392967929106, parameters k is -9.215836390420904 and b is -74.2929610677334\n",
      "Iteration 33, the loss is 154.3389633854644, parameters k is -9.152990046547385 and b is -74.2829610677334\n",
      "Iteration 34, the loss is 153.93399709163745, parameters k is -9.090143702673867 and b is -74.27296106773339\n",
      "Iteration 35, the loss is 153.5290307978105, parameters k is -9.027297358800348 and b is -74.26296106773339\n",
      "Iteration 36, the loss is 153.12406450398385, parameters k is -8.96445101492683 and b is -74.25296106773338\n",
      "Iteration 37, the loss is 152.71909821015694, parameters k is -8.901604671053311 and b is -74.24296106773338\n",
      "Iteration 38, the loss is 152.31413191633013, parameters k is -8.838758327179793 and b is -74.23296106773337\n",
      "Iteration 39, the loss is 151.90916562250317, parameters k is -8.775911983306274 and b is -74.22296106773337\n",
      "Iteration 40, the loss is 151.50419932867632, parameters k is -8.713065639432756 and b is -74.21296106773336\n",
      "Iteration 41, the loss is 151.09923303484945, parameters k is -8.650219295559237 and b is -74.20296106773336\n",
      "Iteration 42, the loss is 150.6942667410226, parameters k is -8.587372951685719 and b is -74.19296106773335\n",
      "Iteration 43, the loss is 150.28930044719576, parameters k is -8.5245266078122 and b is -74.18296106773334\n",
      "Iteration 44, the loss is 149.88433415336902, parameters k is -8.461680263938682 and b is -74.17296106773334\n",
      "Iteration 45, the loss is 149.47936785954198, parameters k is -8.398833920065163 and b is -74.16296106773333\n",
      "Iteration 46, the loss is 149.07440156571548, parameters k is -8.335987576191645 and b is -74.15296106773333\n",
      "Iteration 47, the loss is 148.6694352718883, parameters k is -8.273141232318126 and b is -74.14296106773332\n",
      "Iteration 48, the loss is 148.26446897806153, parameters k is -8.210294888444608 and b is -74.13296106773332\n",
      "Iteration 49, the loss is 147.8595026842346, parameters k is -8.14744854457109 and b is -74.12296106773331\n",
      "Iteration 50, the loss is 147.45453639040795, parameters k is -8.08460220069757 and b is -74.11296106773331\n",
      "Iteration 51, the loss is 147.0495700965811, parameters k is -8.021755856824052 and b is -74.1029610677333\n",
      "Iteration 52, the loss is 146.64460380275418, parameters k is -7.958909512950535 and b is -74.0929610677333\n",
      "Iteration 53, the loss is 146.23963750892725, parameters k is -7.896063169077017 and b is -74.0829610677333\n",
      "Iteration 54, the loss is 145.83467121510026, parameters k is -7.8332168252034995 and b is -74.07296106773329\n",
      "Iteration 55, the loss is 145.42970492127347, parameters k is -7.770370481329982 and b is -74.06296106773328\n",
      "Iteration 56, the loss is 145.02473862744665, parameters k is -7.707524137456464 and b is -74.05296106773328\n",
      "Iteration 57, the loss is 144.61977233361972, parameters k is -7.644677793582947 and b is -74.04296106773327\n",
      "Iteration 58, the loss is 144.21480603979285, parameters k is -7.581831449709429 and b is -74.03296106773327\n",
      "Iteration 59, the loss is 143.80983974596612, parameters k is -7.518985105835911 and b is -74.02296106773326\n",
      "Iteration 60, the loss is 143.40487345213936, parameters k is -7.456138761962394 and b is -74.01296106773326\n",
      "Iteration 61, the loss is 142.99990715831248, parameters k is -7.393292418088876 and b is -74.00296106773325\n",
      "Iteration 62, the loss is 142.59494086448555, parameters k is -7.3304460742153585 and b is -73.99296106773325\n",
      "Iteration 63, the loss is 142.18997457065876, parameters k is -7.267599730341841 and b is -73.98296106773324\n",
      "Iteration 64, the loss is 141.78500827683186, parameters k is -7.204753386468323 and b is -73.97296106773324\n",
      "Iteration 65, the loss is 141.38004198300507, parameters k is -7.141907042594806 and b is -73.96296106773323\n",
      "Iteration 66, the loss is 140.97507568917828, parameters k is -7.079060698721288 and b is -73.95296106773323\n",
      "Iteration 67, the loss is 140.57010939535135, parameters k is -7.0162143548477705 and b is -73.94296106773322\n",
      "Iteration 68, the loss is 140.16514310152454, parameters k is -6.953368010974253 and b is -73.93296106773322\n",
      "Iteration 69, the loss is 139.76017680769777, parameters k is -6.890521667100735 and b is -73.92296106773321\n",
      "Iteration 70, the loss is 139.35521051387076, parameters k is -6.827675323227218 and b is -73.9129610677332\n",
      "Iteration 71, the loss is 138.9502442200439, parameters k is -6.7648289793537 and b is -73.9029610677332\n",
      "Iteration 72, the loss is 138.545277926217, parameters k is -6.701982635480182 and b is -73.8929610677332\n",
      "Iteration 73, the loss is 138.1403116323902, parameters k is -6.639136291606665 and b is -73.88296106773319\n",
      "Iteration 74, the loss is 137.7353453385635, parameters k is -6.576289947733147 and b is -73.87296106773319\n",
      "Iteration 75, the loss is 137.33037904473653, parameters k is -6.5134436038596295 and b is -73.86296106773318\n",
      "Iteration 76, the loss is 136.92541275090974, parameters k is -6.450597259986112 and b is -73.85296106773318\n",
      "Iteration 77, the loss is 136.520446457083, parameters k is -6.387750916112594 and b is -73.84296106773317\n",
      "Iteration 78, the loss is 136.11548016325605, parameters k is -6.324904572239077 and b is -73.83296106773317\n",
      "Iteration 79, the loss is 135.71051386942932, parameters k is -6.262058228365559 and b is -73.82296106773316\n",
      "Iteration 80, the loss is 135.30554757560233, parameters k is -6.199211884492041 and b is -73.81296106773316\n",
      "Iteration 81, the loss is 134.90058128177543, parameters k is -6.136365540618524 and b is -73.80296106773315\n",
      "Iteration 82, the loss is 134.4956149879487, parameters k is -6.073519196745006 and b is -73.79296106773315\n",
      "Iteration 83, the loss is 134.09064869412185, parameters k is -6.010672852871489 and b is -73.78296106773314\n",
      "Iteration 84, the loss is 133.68568240029484, parameters k is -5.947826508997971 and b is -73.77296106773314\n",
      "Iteration 85, the loss is 133.28071610646816, parameters k is -5.884980165124453 and b is -73.76296106773313\n",
      "Iteration 86, the loss is 132.87574981264123, parameters k is -5.822133821250936 and b is -73.75296106773312\n",
      "Iteration 87, the loss is 132.4707835188144, parameters k is -5.759287477377418 and b is -73.74296106773312\n",
      "Iteration 88, the loss is 132.06581722498748, parameters k is -5.6964411335039005 and b is -73.73296106773311\n",
      "Iteration 89, the loss is 131.6608509311606, parameters k is -5.633594789630383 and b is -73.72296106773311\n",
      "Iteration 90, the loss is 131.25588463733393, parameters k is -5.570748445756865 and b is -73.7129610677331\n",
      "Iteration 91, the loss is 130.85091834350692, parameters k is -5.507902101883348 and b is -73.7029610677331\n",
      "Iteration 92, the loss is 130.4459520496801, parameters k is -5.44505575800983 and b is -73.6929610677331\n",
      "Iteration 93, the loss is 130.04098575585326, parameters k is -5.382209414136312 and b is -73.68296106773309\n",
      "Iteration 94, the loss is 129.63601946202647, parameters k is -5.319363070262795 and b is -73.67296106773308\n",
      "Iteration 95, the loss is 129.2310531681995, parameters k is -5.256516726389277 and b is -73.66296106773308\n",
      "Iteration 96, the loss is 128.82608687437272, parameters k is -5.1936703825157595 and b is -73.65296106773307\n",
      "Iteration 97, the loss is 128.42112058054593, parameters k is -5.130824038642242 and b is -73.64296106773307\n",
      "Iteration 98, the loss is 128.01615428671911, parameters k is -5.067977694768724 and b is -73.63296106773306\n",
      "Iteration 99, the loss is 127.6111879928922, parameters k is -5.005131350895207 and b is -73.62296106773306\n",
      "Iteration 100, the loss is 127.20622169906537, parameters k is -4.942285007021689 and b is -73.61296106773305\n",
      "Iteration 101, the loss is 126.80125540523836, parameters k is -4.879438663148171 and b is -73.60296106773305\n",
      "Iteration 102, the loss is 126.39628911141168, parameters k is -4.816592319274654 and b is -73.59296106773304\n",
      "Iteration 103, the loss is 125.9913228175848, parameters k is -4.753745975401136 and b is -73.58296106773304\n",
      "Iteration 104, the loss is 125.58635652375801, parameters k is -4.690899631527619 and b is -73.57296106773303\n",
      "Iteration 105, the loss is 125.18139022993114, parameters k is -4.628053287654101 and b is -73.56296106773303\n",
      "Iteration 106, the loss is 124.77642393610418, parameters k is -4.565206943780583 and b is -73.55296106773302\n",
      "Iteration 107, the loss is 124.37145764227742, parameters k is -4.502360599907066 and b is -73.54296106773302\n",
      "Iteration 108, the loss is 123.96649134845065, parameters k is -4.439514256033548 and b is -73.53296106773301\n",
      "Iteration 109, the loss is 123.56152505462374, parameters k is -4.3766679121600305 and b is -73.52296106773301\n",
      "Iteration 110, the loss is 123.15655876079687, parameters k is -4.313821568286513 and b is -73.512961067733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, the loss is 122.75159246697021, parameters k is -4.250975224412995 and b is -73.502961067733\n",
      "Iteration 112, the loss is 122.34662617314318, parameters k is -4.188128880539478 and b is -73.49296106773299\n",
      "Iteration 113, the loss is 121.94165987931632, parameters k is -4.12528253666596 and b is -73.48296106773299\n",
      "Iteration 114, the loss is 121.53669358548957, parameters k is -4.062436192792442 and b is -73.47296106773298\n",
      "Iteration 115, the loss is 121.13172729166268, parameters k is -3.9995898489189243 and b is -73.46296106773298\n",
      "Iteration 116, the loss is 120.72676099783574, parameters k is -3.9367435050454063 and b is -73.45296106773297\n",
      "Iteration 117, the loss is 120.32179470400898, parameters k is -3.873897161171888 and b is -73.44296106773297\n",
      "Iteration 118, the loss is 119.91682841018195, parameters k is -3.81105081729837 and b is -73.43296106773296\n",
      "Iteration 119, the loss is 119.5118621163553, parameters k is -3.748204473424852 and b is -73.42296106773296\n",
      "Iteration 120, the loss is 119.10689582252832, parameters k is -3.685358129551334 and b is -73.41296106773295\n",
      "Iteration 121, the loss is 118.70192952870158, parameters k is -3.622511785677816 and b is -73.40296106773295\n",
      "Iteration 122, the loss is 118.29696323487461, parameters k is -3.559665441804298 and b is -73.39296106773294\n",
      "Iteration 123, the loss is 117.89199694104778, parameters k is -3.49681909793078 and b is -73.38296106773294\n",
      "Iteration 124, the loss is 117.48703064722096, parameters k is -3.4339727540572618 and b is -73.37296106773293\n",
      "Iteration 125, the loss is 117.08206435339409, parameters k is -3.3711264101837437 and b is -73.36296106773293\n",
      "Iteration 126, the loss is 116.67709805956733, parameters k is -3.3082800663102256 and b is -73.35296106773292\n",
      "Iteration 127, the loss is 116.27213176574047, parameters k is -3.2454337224367076 and b is -73.34296106773292\n",
      "Iteration 128, the loss is 115.86716547191352, parameters k is -3.1825873785631895 and b is -73.33296106773291\n",
      "Iteration 129, the loss is 115.46219917808672, parameters k is -3.1197410346896715 and b is -73.3229610677329\n",
      "Iteration 130, the loss is 115.05723288425985, parameters k is -3.0568946908161534 and b is -73.3129610677329\n",
      "Iteration 131, the loss is 114.65226659043303, parameters k is -2.9940483469426353 and b is -73.3029610677329\n",
      "Iteration 132, the loss is 114.2473002966061, parameters k is -2.9312020030691173 and b is -73.29296106773289\n",
      "Iteration 133, the loss is 113.8423340027792, parameters k is -2.868355659195599 and b is -73.28296106773288\n",
      "Iteration 134, the loss is 113.4373677089524, parameters k is -2.805509315322081 and b is -73.27296106773288\n",
      "Iteration 135, the loss is 113.03240141512562, parameters k is -2.742662971448563 and b is -73.26296106773287\n",
      "Iteration 136, the loss is 112.62743512129877, parameters k is -2.679816627575045 and b is -73.25296106773287\n",
      "Iteration 137, the loss is 112.22246882747191, parameters k is -2.616970283701527 and b is -73.24296106773286\n",
      "Iteration 138, the loss is 111.817502533645, parameters k is -2.554123939828009 and b is -73.23296106773286\n",
      "Iteration 139, the loss is 111.41253623981827, parameters k is -2.491277595954491 and b is -73.22296106773285\n",
      "Iteration 140, the loss is 111.00756994599136, parameters k is -2.4284312520809728 and b is -73.21296106773285\n",
      "Iteration 141, the loss is 110.60260365216462, parameters k is -2.3655849082074547 and b is -73.20296106773284\n",
      "Iteration 142, the loss is 110.19763735833769, parameters k is -2.3027385643339366 and b is -73.19296106773284\n",
      "Iteration 143, the loss is 109.79267106451073, parameters k is -2.2398922204604186 and b is -73.18296106773283\n",
      "Iteration 144, the loss is 109.38770477068387, parameters k is -2.1770458765869005 and b is -73.17296106773283\n",
      "Iteration 145, the loss is 108.98273847685705, parameters k is -2.1141995327133825 and b is -73.16296106773282\n",
      "Iteration 146, the loss is 108.5777721830303, parameters k is -2.0513531888398644 and b is -73.15296106773282\n",
      "Iteration 147, the loss is 108.17280588920335, parameters k is -1.9885068449663466 and b is -73.14296106773281\n",
      "Iteration 148, the loss is 107.76783959537649, parameters k is -1.9256605010928287 and b is -73.13296106773281\n",
      "Iteration 149, the loss is 107.36287330154963, parameters k is -1.8628141572193109 and b is -73.1229610677328\n",
      "Iteration 150, the loss is 106.95790700772288, parameters k is -1.799967813345793 and b is -73.1129610677328\n",
      "Iteration 151, the loss is 106.5529407138959, parameters k is -1.7371214694722752 and b is -73.10296106773279\n",
      "Iteration 152, the loss is 106.14797442006919, parameters k is -1.6742751255987574 and b is -73.09296106773279\n",
      "Iteration 153, the loss is 105.7430081262423, parameters k is -1.6114287817252395 and b is -73.08296106773278\n",
      "Iteration 154, the loss is 105.33804183241539, parameters k is -1.5485824378517217 and b is -73.07296106773278\n",
      "Iteration 155, the loss is 104.93307553858867, parameters k is -1.4857360939782038 and b is -73.06296106773277\n",
      "Iteration 156, the loss is 104.5281092447617, parameters k is -1.422889750104686 and b is -73.05296106773277\n",
      "Iteration 157, the loss is 104.1231429509349, parameters k is -1.3600434062311682 and b is -73.04296106773276\n",
      "Iteration 158, the loss is 103.71817665710796, parameters k is -1.2971970623576503 and b is -73.03296106773276\n",
      "Iteration 159, the loss is 103.31321036328116, parameters k is -1.2343507184841325 and b is -73.02296106773275\n",
      "Iteration 160, the loss is 102.90824406945433, parameters k is -1.1715043746106146 and b is -73.01296106773275\n",
      "Iteration 161, the loss is 102.50327777562752, parameters k is -1.1086580307370968 and b is -73.00296106773274\n",
      "Iteration 162, the loss is 102.09831148180047, parameters k is -1.045811686863579 and b is -72.99296106773274\n",
      "Iteration 163, the loss is 101.6933451879736, parameters k is -0.9829653429900611 and b is -72.98296106773273\n",
      "Iteration 164, the loss is 101.28837889414679, parameters k is -0.9201189991165433 and b is -72.97296106773273\n",
      "Iteration 165, the loss is 100.88341260032, parameters k is -0.8572726552430254 and b is -72.96296106773272\n",
      "Iteration 166, the loss is 100.4784463064933, parameters k is -0.7944263113695076 and b is -72.95296106773272\n",
      "Iteration 167, the loss is 100.07348001266628, parameters k is -0.7315799674959897 and b is -72.94296106773271\n",
      "Iteration 168, the loss is 99.66851371883944, parameters k is -0.6687336236224719 and b is -72.9329610677327\n",
      "Iteration 169, the loss is 99.26354742501266, parameters k is -0.6058872797489541 and b is -72.9229610677327\n",
      "Iteration 170, the loss is 98.85858113118566, parameters k is -0.5430409358754362 and b is -72.9129610677327\n",
      "Iteration 171, the loss is 98.45361483735894, parameters k is -0.4801945920019184 and b is -72.90296106773269\n",
      "Iteration 172, the loss is 98.04864854353222, parameters k is -0.41734824812840055 and b is -72.89296106773268\n",
      "Iteration 173, the loss is 97.64368224970539, parameters k is -0.3545019042548827 and b is -72.88296106773268\n",
      "Iteration 174, the loss is 97.23871595587846, parameters k is -0.2916555603813648 and b is -72.87296106773267\n",
      "Iteration 175, the loss is 96.83374966205162, parameters k is -0.22880921650784694 and b is -72.86296106773267\n",
      "Iteration 176, the loss is 96.42878336822473, parameters k is -0.16596287263432907 and b is -72.85296106773266\n",
      "Iteration 177, the loss is 96.02381707439777, parameters k is -0.1031165287608112 and b is -72.84296106773266\n",
      "Iteration 178, the loss is 95.61885078057104, parameters k is -0.04027018488729334 and b is -72.83296106773265\n",
      "Iteration 179, the loss is 95.21388448674413, parameters k is 0.02257615898622453 and b is -72.82296106773265\n",
      "Iteration 180, the loss is 94.80891819291736, parameters k is 0.0854225028597424 and b is -72.81296106773264\n",
      "Iteration 181, the loss is 94.40395189909046, parameters k is 0.14826884673326027 and b is -72.80296106773264\n",
      "Iteration 182, the loss is 93.99898560526363, parameters k is 0.21111519060677814 and b is -72.79296106773263\n",
      "Iteration 183, the loss is 93.59401931143677, parameters k is 0.27396153448029603 and b is -72.78296106773263\n",
      "Iteration 184, the loss is 93.18905301760985, parameters k is 0.33680787835381387 and b is -72.77296106773262\n",
      "Iteration 185, the loss is 92.78408672378313, parameters k is 0.3996542222273317 and b is -72.76296106773262\n",
      "Iteration 186, the loss is 92.37912042995627, parameters k is 0.46250056610084955 and b is -72.75296106773261\n",
      "Iteration 187, the loss is 91.97415413612931, parameters k is 0.5253469099743674 and b is -72.74296106773261\n",
      "Iteration 188, the loss is 91.56918784230244, parameters k is 0.5881932538478852 and b is -72.7329610677326\n",
      "Iteration 189, the loss is 91.16422154847571, parameters k is 0.6510395977214031 and b is -72.7229610677326\n",
      "Iteration 190, the loss is 90.75925525464878, parameters k is 0.7138859415949209 and b is -72.71296106773259\n",
      "Iteration 191, the loss is 90.35428896082192, parameters k is 0.7767322854684388 and b is -72.70296106773259\n",
      "Iteration 192, the loss is 89.94932266699514, parameters k is 0.8395786293419566 and b is -72.69296106773258\n",
      "Iteration 193, the loss is 89.54435637316828, parameters k is 0.9024249732154744 and b is -72.68296106773258\n",
      "Iteration 194, the loss is 89.13939007934134, parameters k is 0.9652713170889923 and b is -72.67296106773257\n",
      "Iteration 195, the loss is 88.73442378551445, parameters k is 1.0281176609625102 and b is -72.66296106773257\n",
      "Iteration 196, the loss is 88.32945749168766, parameters k is 1.090964004836028 and b is -72.65296106773256\n",
      "Iteration 197, the loss is 87.92449119786085, parameters k is 1.153810348709546 and b is -72.64296106773256\n",
      "Iteration 198, the loss is 87.51952490403404, parameters k is 1.2166566925830637 and b is -72.63296106773255\n",
      "Iteration 199, the loss is 87.11455861020723, parameters k is 1.2795030364565816 and b is -72.62296106773255\n",
      "Iteration 200, the loss is 86.70959231638027, parameters k is 1.3423493803300994 and b is -72.61296106773254\n",
      "Iteration 201, the loss is 86.30462602255342, parameters k is 1.4051957242036173 and b is -72.60296106773254\n",
      "Iteration 202, the loss is 85.89965972872666, parameters k is 1.468042068077135 and b is -72.59296106773253\n",
      "Iteration 203, the loss is 85.49469343489986, parameters k is 1.530888411950653 and b is -72.58296106773253\n",
      "Iteration 204, the loss is 85.08972714107281, parameters k is 1.5937347558241708 and b is -72.57296106773252\n",
      "Iteration 205, the loss is 84.68476084724601, parameters k is 1.6565810996976886 and b is -72.56296106773252\n",
      "Iteration 206, the loss is 84.27979455341926, parameters k is 1.7194274435712065 and b is -72.55296106773251\n",
      "Iteration 207, the loss is 83.87482825959225, parameters k is 1.7822737874447243 and b is -72.5429610677325\n",
      "Iteration 208, the loss is 83.46986196576556, parameters k is 1.8451201313182422 and b is -72.5329610677325\n",
      "Iteration 209, the loss is 83.06489567193864, parameters k is 1.90796647519176 and b is -72.5229610677325\n",
      "Iteration 210, the loss is 82.65992937811194, parameters k is 1.9708128190652778 and b is -72.51296106773249\n",
      "Iteration 211, the loss is 82.25496308428494, parameters k is 2.033659162938796 and b is -72.50296106773249\n",
      "Iteration 212, the loss is 81.84999679045814, parameters k is 2.096505506812314 and b is -72.49296106773248\n",
      "Iteration 213, the loss is 81.44503049663122, parameters k is 2.159351850685832 and b is -72.48296106773248\n",
      "Iteration 214, the loss is 81.04006420280443, parameters k is 2.22219819455935 and b is -72.47296106773247\n",
      "Iteration 215, the loss is 80.63509790897757, parameters k is 2.285044538432868 and b is -72.46296106773246\n",
      "Iteration 216, the loss is 80.23013161515074, parameters k is 2.347890882306386 and b is -72.45296106773246\n",
      "Iteration 217, the loss is 79.82516532132388, parameters k is 2.4107372261799043 and b is -72.44296106773245\n",
      "Iteration 218, the loss is 79.42019902749709, parameters k is 2.4735835700534223 and b is -72.43296106773245\n",
      "Iteration 219, the loss is 79.01523273367017, parameters k is 2.5364299139269404 and b is -72.42296106773244\n",
      "Iteration 220, the loss is 78.61026643984322, parameters k is 2.5992762578004585 and b is -72.41296106773244\n",
      "Iteration 221, the loss is 78.20530014601647, parameters k is 2.6621226016739765 and b is -72.40296106773243\n",
      "Iteration 222, the loss is 77.80033385218964, parameters k is 2.7249689455474946 and b is -72.39296106773243\n",
      "Iteration 223, the loss is 77.39536755836275, parameters k is 2.7878152894210126 and b is -72.38296106773242\n",
      "Iteration 224, the loss is 76.99040126453596, parameters k is 2.8506616332945307 and b is -72.37296106773242\n",
      "Iteration 225, the loss is 76.58543497070916, parameters k is 2.9135079771680488 and b is -72.36296106773241\n",
      "Iteration 226, the loss is 76.18046867688214, parameters k is 2.976354321041567 and b is -72.35296106773241\n",
      "Iteration 227, the loss is 75.7755023830553, parameters k is 3.039200664915085 and b is -72.3429610677324\n",
      "Iteration 228, the loss is 75.37053608922857, parameters k is 3.102047008788603 and b is -72.3329610677324\n",
      "Iteration 229, the loss is 74.96556979540159, parameters k is 3.164893352662121 and b is -72.3229610677324\n",
      "Iteration 230, the loss is 74.56060350157482, parameters k is 3.227739696535639 and b is -72.31296106773239\n",
      "Iteration 231, the loss is 74.1556372077479, parameters k is 3.290586040409157 and b is -72.30296106773238\n",
      "Iteration 232, the loss is 73.75067091392106, parameters k is 3.353432384282675 and b is -72.29296106773238\n",
      "Iteration 233, the loss is 73.34570462009425, parameters k is 3.4162787281561933 and b is -72.28296106773237\n",
      "Iteration 234, the loss is 72.94073832626735, parameters k is 3.4791250720297113 and b is -72.27296106773237\n",
      "Iteration 235, the loss is 72.53577203244058, parameters k is 3.5419714159032294 and b is -72.26296106773236\n",
      "Iteration 236, the loss is 72.13080573861363, parameters k is 3.6048177597767475 and b is -72.25296106773236\n",
      "Iteration 237, the loss is 71.72583944478677, parameters k is 3.6676641036502655 and b is -72.24296106773235\n",
      "Iteration 238, the loss is 71.3208731509599, parameters k is 3.7305104475237836 and b is -72.23296106773235\n",
      "Iteration 239, the loss is 70.91590685713312, parameters k is 3.7933567913973016 and b is -72.22296106773234\n",
      "Iteration 240, the loss is 70.51094056330625, parameters k is 3.8562031352708197 and b is -72.21296106773234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, the loss is 70.10597426947939, parameters k is 3.9190494791443378 and b is -72.20296106773233\n",
      "Iteration 242, the loss is 69.70100797565262, parameters k is 3.981895823017856 and b is -72.19296106773233\n",
      "Iteration 243, the loss is 69.29604168182573, parameters k is 4.044742166891374 and b is -72.18296106773232\n",
      "Iteration 244, the loss is 68.8910753879989, parameters k is 4.1075885107648915 and b is -72.17296106773232\n",
      "Iteration 245, the loss is 68.48610909417205, parameters k is 4.170434854638409 and b is -72.16296106773231\n",
      "Iteration 246, the loss is 68.08114280034513, parameters k is 4.233281198511927 and b is -72.1529610677323\n",
      "Iteration 247, the loss is 67.67617650651829, parameters k is 4.296127542385444 and b is -72.1429610677323\n",
      "Iteration 248, the loss is 67.2712102126915, parameters k is 4.358973886258962 and b is -72.1329610677323\n",
      "Iteration 249, the loss is 66.8662439188646, parameters k is 4.42182023013248 and b is -72.12296106773229\n",
      "Iteration 250, the loss is 66.4612776250378, parameters k is 4.484666574005997 and b is -72.11296106773229\n",
      "Iteration 251, the loss is 66.05631133121102, parameters k is 4.547512917879515 and b is -72.10296106773228\n",
      "Iteration 252, the loss is 65.65134503738405, parameters k is 4.6103592617530325 and b is -72.09296106773228\n",
      "Iteration 253, the loss is 65.24637874355717, parameters k is 4.67320560562655 and b is -72.08296106773227\n",
      "Iteration 254, the loss is 64.84141244973031, parameters k is 4.736051949500068 and b is -72.07296106773227\n",
      "Iteration 255, the loss is 64.43644615590352, parameters k is 4.798898293373585 and b is -72.06296106773226\n",
      "Iteration 256, the loss is 64.03147986207671, parameters k is 4.861744637247103 and b is -72.05296106773226\n",
      "Iteration 257, the loss is 63.626513568249834, parameters k is 4.9245909811206205 and b is -72.04296106773225\n",
      "Iteration 258, the loss is 63.22154727442304, parameters k is 4.987437324994138 and b is -72.03296106773224\n",
      "Iteration 259, the loss is 62.81658098059614, parameters k is 5.050283668867656 and b is -72.02296106773224\n",
      "Iteration 260, the loss is 62.41161468676927, parameters k is 5.113130012741173 and b is -72.01296106773223\n",
      "Iteration 261, the loss is 62.00664839294247, parameters k is 5.175976356614691 and b is -72.00296106773223\n",
      "Iteration 262, the loss is 61.601682099115635, parameters k is 5.238822700488209 and b is -71.99296106773222\n",
      "Iteration 263, the loss is 61.196715805288804, parameters k is 5.301669044361726 and b is -71.98296106773222\n",
      "Iteration 264, the loss is 60.79174951146187, parameters k is 5.364515388235244 and b is -71.97296106773221\n",
      "Iteration 265, the loss is 60.38678321763509, parameters k is 5.4273617321087615 and b is -71.96296106773221\n",
      "Iteration 266, the loss is 59.9818169238082, parameters k is 5.490208075982279 and b is -71.9529610677322\n",
      "Iteration 267, the loss is 59.57685062998137, parameters k is 5.553054419855797 and b is -71.9429610677322\n",
      "Iteration 268, the loss is 59.17188433615448, parameters k is 5.615900763729314 and b is -71.9329610677322\n",
      "Iteration 269, the loss is 58.766918042327625, parameters k is 5.678747107602832 and b is -71.92296106773219\n",
      "Iteration 270, the loss is 58.36195174850078, parameters k is 5.74159345147635 and b is -71.91296106773218\n",
      "Iteration 271, the loss is 57.95698545467398, parameters k is 5.804439795349867 and b is -71.90296106773218\n",
      "Iteration 272, the loss is 57.55201916084708, parameters k is 5.867286139223385 and b is -71.89296106773217\n",
      "Iteration 273, the loss is 57.14705286702026, parameters k is 5.930132483096902 and b is -71.88296106773217\n",
      "Iteration 274, the loss is 56.74208657319338, parameters k is 5.99297882697042 and b is -71.87296106773216\n",
      "Iteration 275, the loss is 56.337120279366545, parameters k is 6.055825170843938 and b is -71.86296106773216\n",
      "Iteration 276, the loss is 55.93215398553973, parameters k is 6.118671514717455 and b is -71.85296106773215\n",
      "Iteration 277, the loss is 55.52718769171283, parameters k is 6.181517858590973 and b is -71.84296106773215\n",
      "Iteration 278, the loss is 55.122221397886044, parameters k is 6.2443642024644905 and b is -71.83296106773214\n",
      "Iteration 279, the loss is 54.717255104059134, parameters k is 6.307210546338008 and b is -71.82296106773214\n",
      "Iteration 280, the loss is 54.312288810232296, parameters k is 6.370056890211526 and b is -71.81296106773213\n",
      "Iteration 281, the loss is 53.90732251640545, parameters k is 6.432903234085043 and b is -71.80296106773213\n",
      "Iteration 282, the loss is 53.502356222578676, parameters k is 6.495749577958561 and b is -71.79296106773212\n",
      "Iteration 283, the loss is 53.097389928751745, parameters k is 6.558595921832079 and b is -71.78296106773212\n",
      "Iteration 284, the loss is 52.692423634924936, parameters k is 6.621442265705596 and b is -71.77296106773211\n",
      "Iteration 285, the loss is 52.28745734109808, parameters k is 6.684288609579114 and b is -71.7629610677321\n",
      "Iteration 286, the loss is 51.882491047271216, parameters k is 6.7471349534526315 and b is -71.7529610677321\n",
      "Iteration 287, the loss is 51.47752475344438, parameters k is 6.809981297326149 and b is -71.7429610677321\n",
      "Iteration 288, the loss is 51.07255845961747, parameters k is 6.872827641199667 and b is -71.73296106773209\n",
      "Iteration 289, the loss is 50.66759216579066, parameters k is 6.935673985073184 and b is -71.72296106773209\n",
      "Iteration 290, the loss is 50.26262587196376, parameters k is 6.998520328946702 and b is -71.71296106773208\n",
      "Iteration 291, the loss is 49.85765957813703, parameters k is 7.06136667282022 and b is -71.70296106773208\n",
      "Iteration 292, the loss is 49.45269328431011, parameters k is 7.124213016693737 and b is -71.69296106773207\n",
      "Iteration 293, the loss is 49.047726990483284, parameters k is 7.187059360567255 and b is -71.68296106773207\n",
      "Iteration 294, the loss is 48.6427606966564, parameters k is 7.249905704440772 and b is -71.67296106773206\n",
      "Iteration 295, the loss is 48.2377944028296, parameters k is 7.31275204831429 and b is -71.66296106773206\n",
      "Iteration 296, the loss is 47.83282810900272, parameters k is 7.375598392187808 and b is -71.65296106773205\n",
      "Iteration 297, the loss is 47.427861815175866, parameters k is 7.438444736061325 and b is -71.64296106773205\n",
      "Iteration 298, the loss is 47.02289552134906, parameters k is 7.501291079934843 and b is -71.63296106773204\n",
      "Iteration 299, the loss is 46.617929227522154, parameters k is 7.5641374238083605 and b is -71.62296106773204\n",
      "Iteration 300, the loss is 46.21296293369529, parameters k is 7.626983767681878 and b is -71.61296106773203\n",
      "Iteration 301, the loss is 45.80799663986845, parameters k is 7.689830111555396 and b is -71.60296106773202\n",
      "Iteration 302, the loss is 45.40303034604161, parameters k is 7.752676455428913 and b is -71.59296106773202\n",
      "Iteration 303, the loss is 44.9980640522148, parameters k is 7.815522799302431 and b is -71.58296106773201\n",
      "Iteration 304, the loss is 44.59309775838792, parameters k is 7.878369143175949 and b is -71.57296106773201\n",
      "Iteration 305, the loss is 44.18813146456108, parameters k is 7.941215487049466 and b is -71.562961067732\n",
      "Iteration 306, the loss is 43.78316517073424, parameters k is 8.004061830922984 and b is -71.552961067732\n",
      "Iteration 307, the loss is 43.3781988769074, parameters k is 8.066908174796502 and b is -71.542961067732\n",
      "Iteration 308, the loss is 42.97323258308054, parameters k is 8.12975451867002 and b is -71.53296106773199\n",
      "Iteration 309, the loss is 42.568266289253664, parameters k is 8.19260086254354 and b is -71.52296106773198\n",
      "Iteration 310, the loss is 42.16329999542684, parameters k is 8.255447206417058 and b is -71.51296106773198\n",
      "Iteration 311, the loss is 41.75833370160001, parameters k is 8.318293550290576 and b is -71.50296106773197\n",
      "Iteration 312, the loss is 41.35336740777309, parameters k is 8.381139894164095 and b is -71.49296106773197\n",
      "Iteration 313, the loss is 40.94840111394627, parameters k is 8.443986238037613 and b is -71.48296106773196\n",
      "Iteration 314, the loss is 40.543434820119415, parameters k is 8.506832581911132 and b is -71.47296106773196\n",
      "Iteration 315, the loss is 40.13846852629258, parameters k is 8.56967892578465 and b is -71.46296106773195\n",
      "Iteration 316, the loss is 39.7335022324657, parameters k is 8.632525269658169 and b is -71.45296106773195\n",
      "Iteration 317, the loss is 39.32853593863886, parameters k is 8.695371613531687 and b is -71.44296106773194\n",
      "Iteration 318, the loss is 38.92356964481202, parameters k is 8.758217957405206 and b is -71.43296106773194\n",
      "Iteration 319, the loss is 38.518603350985174, parameters k is 8.821064301278724 and b is -71.42296106773193\n",
      "Iteration 320, the loss is 38.1136370571583, parameters k is 8.883910645152243 and b is -71.41296106773193\n",
      "Iteration 321, the loss is 37.70867076333148, parameters k is 8.946756989025761 and b is -71.40296106773192\n",
      "Iteration 322, the loss is 37.30370446950456, parameters k is 9.00960333289928 and b is -71.39296106773192\n",
      "Iteration 323, the loss is 36.898738175677714, parameters k is 9.072449676772798 and b is -71.38296106773191\n",
      "Iteration 324, the loss is 36.49377188185086, parameters k is 9.135296020646317 and b is -71.37296106773191\n",
      "Iteration 325, the loss is 36.08880558802403, parameters k is 9.198142364519835 and b is -71.3629610677319\n",
      "Iteration 326, the loss is 35.683839294197156, parameters k is 9.260988708393354 and b is -71.3529610677319\n",
      "Iteration 327, the loss is 35.278873000370304, parameters k is 9.323835052266872 and b is -71.34296106773189\n",
      "Iteration 328, the loss is 34.87390670654342, parameters k is 9.386681396140391 and b is -71.33296106773189\n",
      "Iteration 329, the loss is 34.46894041271665, parameters k is 9.44952774001391 and b is -71.32296106773188\n",
      "Iteration 330, the loss is 34.063974118889774, parameters k is 9.512374083887428 and b is -71.31296106773188\n",
      "Iteration 331, the loss is 33.659007825062936, parameters k is 9.575220427760947 and b is -71.30296106773187\n",
      "Iteration 332, the loss is 33.25404153123601, parameters k is 9.638066771634465 and b is -71.29296106773187\n",
      "Iteration 333, the loss is 32.84907523740915, parameters k is 9.700913115507984 and b is -71.28296106773186\n",
      "Iteration 334, the loss is 32.4441089435823, parameters k is 9.763759459381502 and b is -71.27296106773186\n",
      "Iteration 335, the loss is 32.03914264975548, parameters k is 9.82660580325502 and b is -71.26296106773185\n",
      "Iteration 336, the loss is 31.63417635592859, parameters k is 9.889452147128539 and b is -71.25296106773185\n",
      "Iteration 337, the loss is 31.229210062101775, parameters k is 9.952298491002058 and b is -71.24296106773184\n",
      "Iteration 338, the loss is 30.824243768274894, parameters k is 10.015144834875576 and b is -71.23296106773184\n",
      "Iteration 339, the loss is 30.419277474448048, parameters k is 10.077991178749095 and b is -71.22296106773183\n",
      "Iteration 340, the loss is 30.014311180621178, parameters k is 10.140837522622613 and b is -71.21296106773183\n",
      "Iteration 341, the loss is 29.60934488679435, parameters k is 10.203683866496132 and b is -71.20296106773182\n",
      "Iteration 342, the loss is 29.20437859296752, parameters k is 10.26653021036965 and b is -71.19296106773182\n",
      "Iteration 343, the loss is 28.799412299140652, parameters k is 10.329376554243169 and b is -71.18296106773181\n",
      "Iteration 344, the loss is 28.3944460053138, parameters k is 10.392222898116687 and b is -71.1729610677318\n",
      "Iteration 345, the loss is 27.989479711486936, parameters k is 10.455069241990206 and b is -71.1629610677318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 346, the loss is 27.58451341766008, parameters k is 10.517915585863724 and b is -71.1529610677318\n",
      "Iteration 347, the loss is 27.179547123833206, parameters k is 10.580761929737243 and b is -71.14296106773179\n",
      "Iteration 348, the loss is 26.77623268603237, parameters k is 10.643608273610761 and b is -71.13296106773178\n",
      "Iteration 349, the loss is 26.37569522738492, parameters k is 10.706107581911157 and b is -71.12300059342348\n",
      "Iteration 350, the loss is 25.975157768737507, parameters k is 10.768606890211553 and b is -71.11304011911518\n",
      "Iteration 351, the loss is 25.574620310090058, parameters k is 10.831106198511948 and b is -71.10307964480688\n",
      "Iteration 352, the loss is 25.17408285144266, parameters k is 10.893605506812344 and b is -71.09311917049858\n",
      "Iteration 353, the loss is 24.773545392795228, parameters k is 10.95610481511274 and b is -71.08315869619028\n",
      "Iteration 354, the loss is 24.37300793414781, parameters k is 11.018604123413136 and b is -71.07319822188198\n",
      "Iteration 355, the loss is 23.97247047550039, parameters k is 11.081103431713531 and b is -71.06323774757368\n",
      "Iteration 356, the loss is 23.571933016852977, parameters k is 11.143602740013927 and b is -71.05327727326538\n",
      "Iteration 357, the loss is 23.171395558205585, parameters k is 11.206102048314323 and b is -71.04331679895708\n",
      "Iteration 358, the loss is 22.770858099558133, parameters k is 11.268601356614719 and b is -71.03335632464878\n",
      "Iteration 359, the loss is 22.37032064091072, parameters k is 11.331100664915114 and b is -71.02339585034048\n",
      "Iteration 360, the loss is 21.969783182263317, parameters k is 11.39359997321551 and b is -71.01343537603218\n",
      "Iteration 361, the loss is 21.569245723615886, parameters k is 11.456099281515906 and b is -71.00347490172388\n",
      "Iteration 362, the loss is 21.168708264968476, parameters k is 11.518598589816301 and b is -70.99351442741558\n",
      "Iteration 363, the loss is 20.76840579443138, parameters k is 11.581097898116697 and b is -70.98355395310728\n",
      "Iteration 364, the loss is 20.371608450820936, parameters k is 11.643329143175986 and b is -70.97363300449068\n",
      "Iteration 365, the loss is 19.978706611223128, parameters k is 11.705290664915116 and b is -70.96375158156577\n",
      "Iteration 366, the loss is 19.58889005885979, parameters k is 11.766976771634484 and b is -70.95390968433257\n",
      "Iteration 367, the loss is 19.20232156968321, parameters k is 11.828373826970452 and b is -70.94410731279108\n",
      "Iteration 368, the loss is 18.815753080506678, parameters k is 11.88977088230642 and b is -70.93430494124958\n",
      "Iteration 369, the loss is 18.42918459133013, parameters k is 11.951167937642388 and b is -70.92450256970808\n",
      "Iteration 370, the loss is 18.0429884056908, parameters k is 12.012564992978357 and b is -70.91470019816659\n",
      "Iteration 371, the loss is 17.660076919453505, parameters k is 12.07366983487559 and b is -70.90493735231678\n",
      "Iteration 372, the loss is 17.277165433216236, parameters k is 12.134774676772823 and b is -70.89517450646697\n",
      "Iteration 373, the loss is 16.89576729544733, parameters k is 12.195879518670056 and b is -70.88541166061717\n",
      "Iteration 374, the loss is 16.51603425605565, parameters k is 12.256730052266894 and b is -70.87568834045906\n",
      "Iteration 375, the loss is 16.136555954355, parameters k is 12.317580585863732 and b is -70.86596502030096\n",
      "Iteration 376, the loss is 15.759944522556312, parameters k is 12.37818040799812 and b is -70.85628122583456\n",
      "Iteration 377, the loss is 15.384953894446483, parameters k is 12.438780230132506 and b is -70.84659743136817\n",
      "Iteration 378, the loss is 15.01698459464724, parameters k is 12.498841435666103 and b is -70.83699268828516\n",
      "Iteration 379, the loss is 14.656569568222476, parameters k is 12.558121811160174 and b is -70.82750652227726\n",
      "Iteration 380, the loss is 14.296869978930454, parameters k is 12.617402186654244 and b is -70.81802035626936\n",
      "Iteration 381, the loss is 13.939513219415064, parameters k is 12.676430388235271 and b is -70.80857371595316\n",
      "Iteration 382, the loss is 13.583042736248473, parameters k is 12.735458589816298 and b is -70.79912707563696\n",
      "Iteration 383, the loss is 13.23290376046678, parameters k is 12.794224775587049 and b is -70.78971996101245\n",
      "Iteration 384, the loss is 12.890894901637221, parameters k is 12.851970289421041 and b is -70.78047094915475\n",
      "Iteration 385, the loss is 12.548886042807624, parameters k is 12.909715803255034 and b is -70.77122193729704\n",
      "Iteration 386, the loss is 12.210365128024376, parameters k is 12.967461317089027 and b is -70.76197292543934\n",
      "Iteration 387, the loss is 11.882305895539316, parameters k is 13.02419173210879 and b is -70.75288201634842\n",
      "Iteration 388, the loss is 11.564732579052048, parameters k is 13.079900249895351 and b is -70.74394921002431\n",
      "Iteration 389, the loss is 11.249299147648324, parameters k is 13.135355131318276 and b is -70.7350559293919\n",
      "Iteration 390, the loss is 10.936557035725164, parameters k is 13.190810012741201 and b is -70.7261626487595\n",
      "Iteration 391, the loss is 10.63252807869268, parameters k is 13.245536277563335 and b is -70.71738794520219\n",
      "Iteration 392, the loss is 10.334982665632056, parameters k is 13.29950024989535 and b is -70.70873181871998\n",
      "Iteration 393, the loss is 10.044473698978802, parameters k is 13.352962522622624 and b is -70.70015474362117\n",
      "Iteration 394, the loss is 9.764499710226088, parameters k is 13.405224558195746 and b is -70.69177529698085\n",
      "Iteration 395, the loss is 9.488895894813817, parameters k is 13.457253550290607 and b is -70.68343537603224\n",
      "Iteration 396, the loss is 9.225825671471666, parameters k is 13.508230012741201 and b is -70.67525355785041\n",
      "Iteration 397, the loss is 8.974171952855198, parameters k is 13.557798293373612 and b is -70.6672693681271\n",
      "Iteration 398, the loss is 8.73178651262212, parameters k is 13.606492819065311 and b is -70.65940375547888\n",
      "Iteration 399, the loss is 8.495892363070118, parameters k is 13.654683194559382 and b is -70.65161719421405\n",
      "Iteration 400, the loss is 8.267913335672485, parameters k is 13.701863293373611 and b is -70.64398873571604\n",
      "Iteration 401, the loss is 8.049472750494761, parameters k is 13.748307384282702 and b is -70.63647885429312\n",
      "Iteration 402, the loss is 7.842222036469259, parameters k is 13.7936604870495 and b is -70.62912707563699\n",
      "Iteration 403, the loss is 7.649482197468377, parameters k is 13.837251376377564 and b is -70.62205197682276\n",
      "Iteration 404, the loss is 7.46518008186906, parameters k is 13.87996349100207 and b is -70.61509545508363\n",
      "Iteration 405, the loss is 7.293783581687297, parameters k is 13.921052878353848 and b is -70.6083760874947\n",
      "Iteration 406, the loss is 7.131326414524582, parameters k is 13.96111862934199 and b is -70.60181482267257\n",
      "Iteration 407, the loss is 6.979716067776486, parameters k is 13.999880902069263 and b is -70.59545118630894\n",
      "Iteration 408, the loss is 6.839831616174081, parameters k is 14.037309854638433 and b is -70.5892851784038\n",
      "Iteration 409, the loss is 6.708500364567898, parameters k is 14.073157898116694 and b is -70.58335632464886\n",
      "Iteration 410, the loss is 6.581849382228061, parameters k is 14.108508985073216 and b is -70.57750652227732\n",
      "Iteration 411, the loss is 6.461376453795007, parameters k is 14.143102048314324 and b is -70.57177529698087\n",
      "Iteration 412, the loss is 6.348470326736255, parameters k is 14.176408471239226 and b is -70.56624170014292\n",
      "Iteration 413, the loss is 6.238167593964064, parameters k is 14.209247581911162 and b is -70.56078715468838\n",
      "Iteration 414, the loss is 6.13333747381251, parameters k is 14.241544676772822 and b is -70.55541166061724\n",
      "Iteration 415, the loss is 6.036414945000506, parameters k is 14.272710190606814 and b is -70.5501942693129\n",
      "Iteration 416, the loss is 5.949686452460557, parameters k is 14.30198511155543 and b is -70.54525355785044\n",
      "Iteration 417, the loss is 5.868547965546905, parameters k is 14.330511929737249 and b is -70.54043142346309\n",
      "Iteration 418, the loss is 5.794277239272349, parameters k is 14.357546336851874 and b is -70.53584644322594\n",
      "Iteration 419, the loss is 5.725722118328305, parameters k is 14.38380430127875 and b is -70.53138004006388\n",
      "Iteration 420, the loss is 5.665170603522638, parameters k is 14.408511079934877 and b is -70.52715079105202\n",
      "Iteration 421, the loss is 5.611871145947867, parameters k is 14.431829755824205 and b is -70.52311917049866\n",
      "Iteration 422, the loss is 5.565965654652658, parameters k is 14.45325416293883 and b is -70.5193642297872\n",
      "Iteration 423, the loss is 5.525006435363011, parameters k is 14.4733821668914 and b is -70.51580691753423\n",
      "Iteration 424, the loss is 5.486746656544788, parameters k is 14.492767483096934 and b is -70.51236818235637\n",
      "Iteration 425, the loss is 5.4525810104213805, parameters k is 14.511349716298515 and b is -70.5090480242536\n",
      "Iteration 426, the loss is 5.42256775463601, parameters k is 14.528652819065313 and b is -70.50592549460933\n",
      "Iteration 427, the loss is 5.396490319556176, parameters k is 14.544820230132506 and b is -70.50296106773186\n",
      "Iteration 428, the loss is 5.372972352317285, parameters k is 14.56000159376887 and b is -70.5001547436212\n",
      "Iteration 429, the loss is 5.351275001340224, parameters k is 14.574687581911164 and b is -70.49742747089392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 430, the loss is 5.331745242611519, parameters k is 14.588601079934879 and b is -70.49481877524175\n",
      "Iteration 431, the loss is 5.31310857985781, parameters k is 14.602016277563337 and b is -70.49228913097298\n",
      "Iteration 432, the loss is 5.29473411000566, parameters k is 14.615431475191794 and b is -70.4897594867042\n",
      "Iteration 433, the loss is 5.278794311212228, parameters k is 14.62800497321551 and b is -70.48734841951052\n",
      "Iteration 434, the loss is 5.264908987960781, parameters k is 14.639843846733298 and b is -70.48505592939195\n",
      "Iteration 435, the loss is 5.253785707935262, parameters k is 14.650406514717487 and b is -70.48296106773186\n",
      "Iteration 436, the loss is 5.243794150222918, parameters k is 14.660441238037645 and b is -70.48094525745519\n",
      "Iteration 437, the loss is 5.235611019819156, parameters k is 14.669739755824207 and b is -70.47904802425361\n",
      "Iteration 438, the loss is 5.229767184518187, parameters k is 14.677513846733298 and b is -70.47738794520224\n",
      "Iteration 439, the loss is 5.224962929361842, parameters k is 14.684280427760966 and b is -70.47588596891765\n",
      "Iteration 440, the loss is 5.2203574508876205, parameters k is 14.691047008788633 and b is -70.47438399263307\n",
      "Iteration 441, the loss is 5.216574771991489, parameters k is 14.697333075982309 and b is -70.47296106773187\n",
      "Iteration 442, the loss is 5.213704056127107, parameters k is 14.702628708393377 and b is -70.47169624559749\n",
      "Iteration 443, the loss is 5.2110286598693545, parameters k is 14.707653906021836 and b is -70.4704709491548\n",
      "Iteration 444, the loss is 5.2084329889919845, parameters k is 14.712679103650295 and b is -70.46924565271212\n",
      "Iteration 445, the loss is 5.206008620554695, parameters k is 14.717457977168081 and b is -70.46805988196112\n",
      "Iteration 446, the loss is 5.203672670101037, parameters k is 14.722236850685867 and b is -70.46687411121013\n",
      "Iteration 447, the loss is 5.201483997906955, parameters k is 14.726807147128556 and b is -70.46572786615084\n",
      "Iteration 448, the loss is 5.1996268313750456, parameters k is 14.731166020646342 and b is -70.46462114678326\n",
      "Iteration 449, the loss is 5.19806184533398, parameters k is 14.734986198511955 and b is -70.46359347879907\n",
      "Iteration 450, the loss is 5.1964968592929095, parameters k is 14.738806376377568 and b is -70.46256581081488\n",
      "Iteration 451, the loss is 5.194931873251849, parameters k is 14.74262655424318 and b is -70.46153814283069\n",
      "Iteration 452, the loss is 5.193366887210788, parameters k is 14.746446732108794 and b is -70.4605104748465\n",
      "Iteration 453, the loss is 5.191880674985405, parameters k is 14.750266909974407 and b is -70.45948280686231\n",
      "Iteration 454, the loss is 5.190572512063109, parameters k is 14.753840882306422 and b is -70.45849466456983\n",
      "Iteration 455, the loss is 5.1893800616232895, parameters k is 14.7571612182748 and b is -70.45754604796903\n",
      "Iteration 456, the loss is 5.188187611183468, parameters k is 14.76048155424318 and b is -70.45659743136824\n",
      "Iteration 457, the loss is 5.1870931743769395, parameters k is 14.763801890211559 and b is -70.45564881476744\n",
      "Iteration 458, the loss is 5.186207527664403, parameters k is 14.766647996930926 and b is -70.45477924955006\n",
      "Iteration 459, the loss is 5.185394599597581, parameters k is 14.769494103650294 and b is -70.45390968433267\n",
      "Iteration 460, the loss is 5.184768158528012, parameters k is 14.771868866496144 and b is -70.45311917049868\n",
      "Iteration 461, the loss is 5.184141717458426, parameters k is 14.774243629341994 and b is -70.45232865666469\n",
      "Iteration 462, the loss is 5.183515276388853, parameters k is 14.776618392187844 and b is -70.4515381428307\n",
      "Iteration 463, the loss is 5.182888835319286, parameters k is 14.778993155033694 and b is -70.45074762899671\n",
      "Iteration 464, the loss is 5.182298046284171, parameters k is 14.781367917879544 and b is -70.44995711516272\n",
      "Iteration 465, the loss is 5.181791964928385, parameters k is 14.783488491002075 and b is -70.44920612702043\n",
      "Iteration 466, the loss is 5.18131605717003, parameters k is 14.785609064124605 and b is -70.44845513887815\n",
      "Iteration 467, the loss is 5.18096692991988, parameters k is 14.787494775587056 and b is -70.44774367642755\n",
      "Iteration 468, the loss is 5.1807451814513445, parameters k is 14.788842937642393 and b is -70.44711126536036\n",
      "Iteration 469, the loss is 5.180523432982804, parameters k is 14.79019109969773 and b is -70.44647885429318\n",
      "Iteration 470, the loss is 5.18030168451427, parameters k is 14.791539261753066 and b is -70.44584644322599\n",
      "Iteration 471, the loss is 5.18009505060016, parameters k is 14.792887423808402 and b is -70.4452140321588\n",
      "Iteration 472, the loss is 5.179942885276065, parameters k is 14.793990170843974 and b is -70.4446211467833\n",
      "Iteration 473, the loss is 5.1798557477951475, parameters k is 14.794870506812353 and b is -70.44406778709951\n",
      "Iteration 474, the loss is 5.179790173986742, parameters k is 14.79549637637757 and b is -70.44355395310741\n",
      "Iteration 475, the loss is 5.179724600178331, parameters k is 14.796122245942788 and b is -70.44304011911531\n",
      "Iteration 476, the loss is 5.179659026369924, parameters k is 14.796748115508006 and b is -70.44252628512321\n",
      "Iteration 477, the loss is 5.17959345256151, parameters k is 14.797373985073223 and b is -70.4420124511311\n",
      "Iteration 478, the loss is 5.179527878753102, parameters k is 14.79799985463844 and b is -70.441498617139\n",
      "Iteration 479, the loss is 5.179462304944691, parameters k is 14.798625724203658 and b is -70.4409847831469\n",
      "Iteration 480, the loss is 5.179396731136282, parameters k is 14.799251593768876 and b is -70.4404709491548\n",
      "Iteration 481, the loss is 5.179331157327872, parameters k is 14.799877463334093 and b is -70.4399571151627\n",
      "Iteration 482, the loss is 5.1792655835194585, parameters k is 14.80050333289931 and b is -70.4394432811706\n",
      "Iteration 483, the loss is 5.179200009711052, parameters k is 14.801129202464528 and b is -70.4389294471785\n",
      "Iteration 484, the loss is 5.1791344359026406, parameters k is 14.801755072029746 and b is -70.4384156131864\n",
      "Iteration 485, the loss is 5.179068862094235, parameters k is 14.802380941594963 and b is -70.43790177919429\n",
      "Iteration 486, the loss is 5.179003288285823, parameters k is 14.80300681116018 and b is -70.43738794520219\n",
      "Iteration 487, the loss is 5.178952162578312, parameters k is 14.803632680725398 and b is -70.43687411121009\n",
      "Iteration 488, the loss is 5.178915499295609, parameters k is 14.804009064124608 and b is -70.43639980290969\n",
      "Iteration 489, the loss is 5.178878836012909, parameters k is 14.804385447523819 and b is -70.4359254946093\n",
      "Iteration 490, the loss is 5.1788421727302, parameters k is 14.804761830923029 and b is -70.4354511863089\n",
      "Iteration 491, the loss is 5.1788055094475025, parameters k is 14.80513821432224 and b is -70.4349768780085\n",
      "Iteration 492, the loss is 5.1787688461648, parameters k is 14.80551459772145 and b is -70.4345025697081\n",
      "Iteration 493, the loss is 5.1787321828821, parameters k is 14.80589098112066 and b is -70.4340282614077\n",
      "Iteration 494, the loss is 5.178695519599399, parameters k is 14.80626736451987 and b is -70.4335539531073\n",
      "Iteration 495, the loss is 5.178658856316691, parameters k is 14.80664374791908 and b is -70.43307964480691\n",
      "Iteration 496, the loss is 5.1786221930339975, parameters k is 14.807020131318291 and b is -70.43260533650651\n",
      "Iteration 497, the loss is 5.178585529751285, parameters k is 14.807396514717501 and b is -70.43213102820611\n",
      "Iteration 498, the loss is 5.17854886646858, parameters k is 14.807772898116712 and b is -70.43165671990572\n",
      "Iteration 499, the loss is 5.178512203185879, parameters k is 14.808149281515922 and b is -70.43118241160532\n",
      "Iteration 500, the loss is 5.178475539903185, parameters k is 14.808525664915132 and b is -70.43070810330492\n",
      "Iteration 501, the loss is 5.178438876620476, parameters k is 14.808902048314343 and b is -70.43023379500453\n",
      "Iteration 502, the loss is 5.178402213337778, parameters k is 14.809278431713553 and b is -70.42975948670413\n",
      "Iteration 503, the loss is 5.178365550055075, parameters k is 14.809654815112763 and b is -70.42928517840373\n",
      "Iteration 504, the loss is 5.178328886772374, parameters k is 14.810031198511973 and b is -70.42881087010333\n",
      "Iteration 505, the loss is 5.178292223489667, parameters k is 14.810407581911184 and b is -70.42833656180294\n",
      "Iteration 506, the loss is 5.178255560206966, parameters k is 14.810783965310394 and b is -70.42786225350254\n",
      "Iteration 507, the loss is 5.178218896924262, parameters k is 14.811160348709604 and b is -70.42738794520214\n",
      "Iteration 508, the loss is 5.178182233641559, parameters k is 14.811536732108815 and b is -70.42691363690174\n",
      "Iteration 509, the loss is 5.178156326227124, parameters k is 14.811913115508025 and b is -70.42643932860135\n",
      "Iteration 510, the loss is 5.178136969807517, parameters k is 14.811980407998144 and b is -70.42600454599265\n",
      "Iteration 511, the loss is 5.1781176133879185, parameters k is 14.812047700488263 and b is -70.42556976338396\n",
      "Iteration 512, the loss is 5.178098256968309, parameters k is 14.812114992978382 and b is -70.42513498077527\n",
      "Iteration 513, the loss is 5.178078900548705, parameters k is 14.8121822854685 and b is -70.42470019816658\n",
      "Iteration 514, the loss is 5.178059544129099, parameters k is 14.81224957795862 and b is -70.42426541555788\n",
      "Iteration 515, the loss is 5.17804018770949, parameters k is 14.812316870448738 and b is -70.42383063294919\n",
      "Iteration 516, the loss is 5.178020831289893, parameters k is 14.812384162938857 and b is -70.4233958503405\n",
      "Iteration 517, the loss is 5.1780014748702845, parameters k is 14.812451455428976 and b is -70.4229610677318\n",
      "Iteration 518, the loss is 5.177985048832271, parameters k is 14.812518747919095 and b is -70.42252628512311\n",
      "Iteration 519, the loss is 5.177966504633675, parameters k is 14.812285249895378 and b is -70.42213102820611\n",
      "Iteration 520, the loss is 5.177948360079508, parameters k is 14.812352542385497 and b is -70.42169624559742\n",
      "Iteration 521, the loss is 5.177931534397071, parameters k is 14.812119044361781 and b is -70.42130098868041\n",
      "Iteration 522, the loss is 5.177912177977467, parameters k is 14.8121863368519 and b is -70.42086620607172\n",
      "Iteration 523, the loss is 5.177896057509754, parameters k is 14.812253629342019 and b is -70.42043142346303\n",
      "Iteration 524, the loss is 5.177877207740855, parameters k is 14.812020131318302 and b is -70.42003616654603\n",
      "Iteration 525, the loss is 5.177859368757002, parameters k is 14.812087423808421 and b is -70.41960138393733\n",
      "Iteration 526, the loss is 5.177842237504248, parameters k is 14.811853925784705 and b is -70.41920612702033\n",
      "Iteration 527, the loss is 5.1778228810846505, parameters k is 14.811921218274824 and b is -70.41877134441164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 528, the loss is 5.177807066187245, parameters k is 14.811988510764943 and b is -70.41833656180295\n",
      "Iteration 529, the loss is 5.177787910848041, parameters k is 14.811755012741227 and b is -70.41794130488594\n",
      "Iteration 530, the loss is 5.177770377434494, parameters k is 14.811822305231345 and b is -70.41750652227725\n",
      "Iteration 531, the loss is 5.177752940611432, parameters k is 14.81158880720763 and b is -70.41711126536025\n",
      "Iteration 532, the loss is 5.177733688681741, parameters k is 14.811656099697748 and b is -70.41667648275155\n",
      "Iteration 533, the loss is 5.177717970374817, parameters k is 14.811422601674032 and b is -70.41628122583455\n",
      "Iteration 534, the loss is 5.177698613955219, parameters k is 14.81148989416415 and b is -70.41584644322586\n",
      "Iteration 535, the loss is 5.177681386111986, parameters k is 14.81155718665427 and b is -70.41541166061717\n",
      "Iteration 536, the loss is 5.177663643718602, parameters k is 14.811323688630553 and b is -70.41501640370016\n",
      "Iteration 537, the loss is 5.17764469735923, parameters k is 14.811390981120672 and b is -70.41458162109147\n",
      "Iteration 538, the loss is 5.1776286734820065, parameters k is 14.811157483096956 and b is -70.41418636417447\n",
      "Iteration 539, the loss is 5.177609317062403, parameters k is 14.811224775587075 and b is -70.41375158156578\n",
      "Iteration 540, the loss is 5.177592394789472, parameters k is 14.811292068077194 and b is -70.41331679895708\n",
      "Iteration 541, the loss is 5.17757434682579, parameters k is 14.811058570053477 and b is -70.41292154204008\n",
      "Iteration 542, the loss is 5.177555706036721, parameters k is 14.811125862543596 and b is -70.41248675943139\n",
      "Iteration 543, the loss is 5.17753937658918, parameters k is 14.81089236451988 and b is -70.41209150251439\n",
      "Iteration 544, the loss is 5.177520020169576, parameters k is 14.810959657009999 and b is -70.4116567199057\n",
      "Iteration 545, the loss is 5.177503403466966, parameters k is 14.811026949500118 and b is -70.411221937297\n",
      "Iteration 546, the loss is 5.177485049932967, parameters k is 14.810793451476401 and b is -70.41082668038\n",
      "Iteration 547, the loss is 5.177466714714211, parameters k is 14.81086074396652 and b is -70.4103918977713\n",
      "Iteration 548, the loss is 5.177450079696357, parameters k is 14.810627245942804 and b is -70.4099966408543\n",
      "Iteration 549, the loss is 5.177430723276756, parameters k is 14.810694538432923 and b is -70.40956185824561\n",
      "Iteration 550, the loss is 5.177414412144454, parameters k is 14.810761830923042 and b is -70.40912707563692\n",
      "Iteration 551, the loss is 5.177395753040151, parameters k is 14.810528332899326 and b is -70.40873181871991\n",
      "Iteration 552, the loss is 5.1773777233917, parameters k is 14.810595625389444 and b is -70.40829703611122\n",
      "Iteration 553, the loss is 5.17736078280354, parameters k is 14.810362127365728 and b is -70.40790177919422\n",
      "Iteration 554, the loss is 5.177341426383934, parameters k is 14.810429419855847 and b is -70.40746699658553\n",
      "Iteration 555, the loss is 5.177325420821943, parameters k is 14.810496712345966 and b is -70.40703221397683\n",
      "Iteration 556, the loss is 5.17730645614733, parameters k is 14.81026321432225 and b is -70.40663695705983\n",
      "Iteration 557, the loss is 5.177288732069192, parameters k is 14.810330506812369 and b is -70.40620217445114\n",
      "Iteration 558, the loss is 5.1772714859107225, parameters k is 14.810097008788652 and b is -70.40580691753414\n",
      "Iteration 559, the loss is 5.177252129491118, parameters k is 14.810164301278771 and b is -70.40537213492544\n",
      "Iteration 560, the loss is 5.177236429499438, parameters k is 14.81023159376889 and b is -70.40493735231675\n",
      "Iteration 561, the loss is 5.177217159254509, parameters k is 14.809998095745174 and b is -70.40454209539975\n",
      "Iteration 562, the loss is 5.177199740746678, parameters k is 14.810065388235293 and b is -70.40410731279106\n",
      "Iteration 563, the loss is 5.177182189017904, parameters k is 14.809831890211576 and b is -70.40371205587405\n",
      "Iteration 564, the loss is 5.177163051993927, parameters k is 14.809899182701695 and b is -70.40327727326536\n",
      "Iteration 565, the loss is 5.177147218781298, parameters k is 14.809665684677979 and b is -70.40288201634836\n",
      "Iteration 566, the loss is 5.177127862361691, parameters k is 14.809732977168098 and b is -70.40244723373966\n",
      "Iteration 567, the loss is 5.177110749424172, parameters k is 14.809800269658217 and b is -70.40201245113097\n",
      "Iteration 568, the loss is 5.177092892125087, parameters k is 14.8095667716345 and b is -70.40161719421397\n",
      "Iteration 569, the loss is 5.177074060671418, parameters k is 14.80963406412462 and b is -70.40118241160528\n",
      "Iteration 570, the loss is 5.177057921888471, parameters k is 14.809400566100903 and b is -70.40078715468827\n",
      "Iteration 571, the loss is 5.177038565468876, parameters k is 14.809467858591022 and b is -70.40035237207958\n",
      "Iteration 572, the loss is 5.177021758101664, parameters k is 14.80953515108114 and b is -70.39991758947089\n",
      "Iteration 573, the loss is 5.177003595232257, parameters k is 14.809301653057425 and b is -70.39952233255389\n",
      "Iteration 574, the loss is 5.176985069348911, parameters k is 14.809368945547543 and b is -70.3990875499452\n",
      "Iteration 575, the loss is 5.176968624995657, parameters k is 14.809135447523827 and b is -70.39869229302819\n",
      "Iteration 576, the loss is 5.176949268576051, parameters k is 14.809202740013946 and b is -70.3982575104195\n",
      "Iteration 577, the loss is 5.176932766779154, parameters k is 14.809270032504065 and b is -70.3978227278108\n",
      "Iteration 578, the loss is 5.176914298339442, parameters k is 14.809036534480349 and b is -70.3974274708938\n",
      "Iteration 579, the loss is 5.176896078026404, parameters k is 14.809103826970468 and b is -70.39699268828511\n",
      "Iteration 580, the loss is 5.176879328102836, parameters k is 14.808870328946751 and b is -70.39659743136811\n",
      "Iteration 581, the loss is 5.176859971683233, parameters k is 14.80893762143687 and b is -70.39616264875941\n",
      "Iteration 582, the loss is 5.176843775456645, parameters k is 14.809004913926989 and b is -70.39572786615072\n",
      "Iteration 583, the loss is 5.176825001446626, parameters k is 14.808771415903273 and b is -70.39533260923372\n",
      "Iteration 584, the loss is 5.1768070867038904, parameters k is 14.808838708393392 and b is -70.39489782662503\n",
      "Iteration 585, the loss is 5.176790031210012, parameters k is 14.808605210369675 and b is -70.39450256970802\n",
      "Iteration 586, the loss is 5.1767706747904105, parameters k is 14.808672502859794 and b is -70.39406778709933\n",
      "Iteration 587, the loss is 5.176754784134141, parameters k is 14.808739795349913 and b is -70.39363300449064\n",
      "Iteration 588, the loss is 5.176735704553802, parameters k is 14.808506297326197 and b is -70.39323774757364\n",
      "Iteration 589, the loss is 5.1767180953813865, parameters k is 14.808573589816316 and b is -70.39280296496494\n",
      "Iteration 590, the loss is 5.176700734317197, parameters k is 14.8083400917926 and b is -70.39240770804794\n",
      "Iteration 591, the loss is 5.1766814066286235, parameters k is 14.808407384282718 and b is -70.39197292543925\n",
      "Iteration 592, the loss is 5.176665764080587, parameters k is 14.808173886259002 and b is -70.39157766852225\n",
      "Iteration 593, the loss is 5.176646407660981, parameters k is 14.808241178749121 and b is -70.39114288591355\n",
      "Iteration 594, the loss is 5.1766291040588674, parameters k is 14.80830847123924 and b is -70.39070810330486\n",
      "Iteration 595, the loss is 5.176611437424375, parameters k is 14.808074973215524 and b is -70.39031284638786\n",
      "Iteration 596, the loss is 5.1765924153061205, parameters k is 14.808142265705643 and b is -70.38987806377916\n",
      "Iteration 597, the loss is 5.176576467187769, parameters k is 14.807908767681926 and b is -70.38948280686216\n",
      "Iteration 598, the loss is 5.176557110768157, parameters k is 14.807976060172045 and b is -70.38904802425347\n",
      "Iteration 599, the loss is 5.176540112736366, parameters k is 14.808043352662164 and b is -70.38861324164478\n",
      "Iteration 600, the loss is 5.176522140531555, parameters k is 14.807809854638448 and b is -70.38821798472777\n",
      "Iteration 601, the loss is 5.176503423983605, parameters k is 14.807877147128567 and b is -70.38778320211908\n",
      "Iteration 602, the loss is 5.176487170294949, parameters k is 14.80764364910485 and b is -70.38738794520208\n",
      "Iteration 603, the loss is 5.176467813875347, parameters k is 14.80771094159497 and b is -70.38695316259339\n",
      "Iteration 604, the loss is 5.176451121413853, parameters k is 14.807778234085088 and b is -70.3865183799847\n",
      "Iteration 605, the loss is 5.176432843638737, parameters k is 14.807544736061372 and b is -70.38612312306769\n",
      "Iteration 606, the loss is 5.176414432661096, parameters k is 14.80761202855149 and b is -70.385688340459\n",
      "Iteration 607, the loss is 5.176397873402126, parameters k is 14.807378530527775 and b is -70.385293083542\n",
      "Iteration 608, the loss is 5.176378516982525, parameters k is 14.807445823017893 and b is -70.3848583009333\n",
      "Iteration 609, the loss is 5.17636213009134, parameters k is 14.807513115508012 and b is -70.38442351832461\n",
      "Iteration 610, the loss is 5.176343546745914, parameters k is 14.807279617484296 and b is -70.38402826140761\n",
      "Iteration 611, the loss is 5.176325441338586, parameters k is 14.807346909974415 and b is -70.38359347879891\n",
      "Iteration 612, the loss is 5.176308576509307, parameters k is 14.807113411950699 and b is -70.38319822188191\n",
      "Iteration 613, the loss is 5.176289220089703, parameters k is 14.807180704440817 and b is -70.38276343927322\n",
      "Iteration 614, the loss is 5.176273138768836, parameters k is 14.807247996930936 and b is -70.38232865666453\n",
      "Iteration 615, the loss is 5.176254249853095, parameters k is 14.80701449890722 and b is -70.38193339974752\n",
      "Iteration 616, the loss is 5.17623645001608, parameters k is 14.807081791397339 and b is -70.38149861713883\n",
      "Iteration 617, the loss is 5.176219279616492, parameters k is 14.806848293373623 and b is -70.38110336022183\n",
      "Iteration 618, the loss is 5.176199923196883, parameters k is 14.806915585863742 and b is -70.38066857761314\n",
      "Iteration 619, the loss is 5.176184147446321, parameters k is 14.80698287835386 and b is -70.38023379500444\n",
      "Iteration 620, the loss is 5.176164952960274, parameters k is 14.806749380330144 and b is -70.37983853808744\n",
      "Iteration 621, the loss is 5.176147458693568, parameters k is 14.806816672820263 and b is -70.37940375547875\n",
      "Iteration 622, the loss is 5.176129982723672, parameters k is 14.806583174796547 and b is -70.37900849856175\n",
      "Iteration 623, the loss is 5.176110769940821, parameters k is 14.806650467286666 and b is -70.37857371595305\n",
      "Iteration 624, the loss is 5.176095012487059, parameters k is 14.80641696926295 and b is -70.37817845903605\n",
      "Iteration 625, the loss is 5.176075656067454, parameters k is 14.806484261753068 and b is -70.37774367642736\n",
      "Iteration 626, the loss is 5.176058467371058, parameters k is 14.806551554243187 and b is -70.37730889381866\n",
      "Iteration 627, the loss is 5.17604068583085, parameters k is 14.806318056219471 and b is -70.37691363690166\n",
      "Iteration 628, the loss is 5.176021778618309, parameters k is 14.80638534870959 and b is -70.37647885429297\n",
      "Iteration 629, the loss is 5.1760057155942425, parameters k is 14.806151850685874 and b is -70.37608359737597\n",
      "Iteration 630, the loss is 5.175986359174637, parameters k is 14.806219143175992 and b is -70.37564881476727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 631, the loss is 5.175969476048552, parameters k is 14.806286435666111 and b is -70.37521403215858\n",
      "Iteration 632, the loss is 5.175951388938024, parameters k is 14.806052937642395 and b is -70.37481877524158\n",
      "Iteration 633, the loss is 5.175932787295797, parameters k is 14.806120230132514 and b is -70.37438399263289\n",
      "Iteration 634, the loss is 5.1759164187014255, parameters k is 14.805886732108798 and b is -70.37398873571588\n",
      "Iteration 635, the loss is 5.175897062281819, parameters k is 14.805954024598917 and b is -70.37355395310719\n",
      "Iteration 636, the loss is 5.175880484726047, parameters k is 14.806021317089035 and b is -70.3731191704985\n",
      "Iteration 637, the loss is 5.175862092045207, parameters k is 14.80578781906532 and b is -70.3727239135815\n",
      "Iteration 638, the loss is 5.175843795973286, parameters k is 14.805855111555438 and b is -70.3722891309728\n",
      "Iteration 639, the loss is 5.175827121808605, parameters k is 14.805621613531722 and b is -70.3718938740558\n",
      "Iteration 640, the loss is 5.175807765388999, parameters k is 14.80568890602184 and b is -70.37145909144711\n",
      "Iteration 641, the loss is 5.175791493403527, parameters k is 14.80575619851196 and b is -70.37102430883841\n",
      "Iteration 642, the loss is 5.175772795152384, parameters k is 14.805522700488243 and b is -70.37062905192141\n",
      "Iteration 643, the loss is 5.175754804650779, parameters k is 14.805589992978362 and b is -70.37019426931272\n",
      "Iteration 644, the loss is 5.175737824915781, parameters k is 14.805356494954646 and b is -70.36979901239572\n",
      "Iteration 645, the loss is 5.1757184684961715, parameters k is 14.805423787444765 and b is -70.36936422978702\n",
      "Iteration 646, the loss is 5.1757025020810214, parameters k is 14.805491079934884 and b is -70.36892944717833\n",
      "Iteration 647, the loss is 5.17568349825957, parameters k is 14.805257581911167 and b is -70.36853419026133\n",
      "Iteration 648, the loss is 5.175665813328268, parameters k is 14.805324874401286 and b is -70.36809940765264\n",
      "Iteration 649, the loss is 5.17564852802296, parameters k is 14.80509137637757 and b is -70.36770415073563\n",
      "Iteration 650, the loss is 5.175629171603356, parameters k is 14.805158668867689 and b is -70.36726936812694\n",
      "Iteration 651, the loss is 5.175613510758513, parameters k is 14.805225961357808 and b is -70.36683458551825\n",
      "Iteration 652, the loss is 5.175594201366747, parameters k is 14.804992463334091 and b is -70.36643932860125\n",
      "Iteration 653, the loss is 5.175576822005761, parameters k is 14.80505975582421 and b is -70.36600454599255\n",
      "Iteration 654, the loss is 5.175559231130137, parameters k is 14.804826257800494 and b is -70.36560928907555\n",
      "Iteration 655, the loss is 5.175540133253007, parameters k is 14.804893550290613 and b is -70.36517450646686\n",
      "Iteration 656, the loss is 5.175524260893535, parameters k is 14.804660052266897 and b is -70.36477924954985\n",
      "Iteration 657, the loss is 5.175504904473928, parameters k is 14.804727344757016 and b is -70.36434446694116\n",
      "Iteration 658, the loss is 5.175487830683253, parameters k is 14.804794637247134 and b is -70.36390968433247\n",
      "Iteration 659, the loss is 5.17546993423732, parameters k is 14.804561139223418 and b is -70.36351442741547\n",
      "Iteration 660, the loss is 5.175451141930493, parameters k is 14.804628431713537 and b is -70.36307964480677\n",
      "Iteration 661, the loss is 5.175434964000713, parameters k is 14.80439493368982 and b is -70.36268438788977\n",
      "Iteration 662, the loss is 5.175415607581104, parameters k is 14.80446222617994 and b is -70.36224960528108\n",
      "Iteration 663, the loss is 5.175398839360743, parameters k is 14.804529518670059 and b is -70.36181482267239\n",
      "Iteration 664, the loss is 5.175380637344498, parameters k is 14.804296020646342 and b is -70.36141956575538\n",
      "Iteration 665, the loss is 5.17536215060799, parameters k is 14.804363313136461 and b is -70.36098478314669\n",
      "Iteration 666, the loss is 5.175345667107896, parameters k is 14.804129815112745 and b is -70.36058952622969\n",
      "Iteration 667, the loss is 5.1753263106882885, parameters k is 14.804197107602864 and b is -70.360154743621\n",
      "Iteration 668, the loss is 5.175309848038234, parameters k is 14.804264400092983 and b is -70.3597199610123\n",
      "Iteration 669, the loss is 5.175291340451681, parameters k is 14.804030902069266 and b is -70.3593247040953\n",
      "Iteration 670, the loss is 5.17527315928548, parameters k is 14.804098194559385 and b is -70.35888992148661\n",
      "Iteration 671, the loss is 5.175256370215075, parameters k is 14.803864696535669 and b is -70.3584946645696\n",
      "Iteration 672, the loss is 5.175237013795469, parameters k is 14.803931989025788 and b is -70.35805988196091\n",
      "Iteration 673, the loss is 5.175220856715718, parameters k is 14.803999281515907 and b is -70.35762509935222\n",
      "Iteration 674, the loss is 5.175202043558861, parameters k is 14.80376578349219 and b is -70.35722984243522\n",
      "Iteration 675, the loss is 5.175184167962967, parameters k is 14.80383307598231 and b is -70.35679505982652\n",
      "Iteration 676, the loss is 5.1751670733222594, parameters k is 14.803599577958593 and b is -70.35639980290952\n",
      "Iteration 677, the loss is 5.175147716902651, parameters k is 14.803666870448712 and b is -70.35596502030083\n",
      "Iteration 678, the loss is 5.1751318653932135, parameters k is 14.80373416293883 and b is -70.35553023769214\n",
      "Iteration 679, the loss is 5.17511274666604, parameters k is 14.803500664915115 and b is -70.35513498077513\n",
      "Iteration 680, the loss is 5.175095176640463, parameters k is 14.803567957405233 and b is -70.35470019816644\n",
      "Iteration 681, the loss is 5.175077776429429, parameters k is 14.803334459381517 and b is -70.35430494124944\n",
      "Iteration 682, the loss is 5.175058487887702, parameters k is 14.803401751871636 and b is -70.35387015864075\n",
      "Iteration 683, the loss is 5.1750428061928355, parameters k is 14.80316825384792 and b is -70.35347490172374\n",
      "Iteration 684, the loss is 5.175023449773223, parameters k is 14.803235546338039 and b is -70.35304011911505\n",
      "Iteration 685, the loss is 5.1750061853179465, parameters k is 14.803302838828158 and b is -70.35260533650636\n",
      "Iteration 686, the loss is 5.174988479536613, parameters k is 14.803069340804441 and b is -70.35221007958936\n",
      "Iteration 687, the loss is 5.174969496565186, parameters k is 14.80313663329456 and b is -70.35177529698066\n",
      "Iteration 688, the loss is 5.174953509300005, parameters k is 14.802903135270844 and b is -70.35138004006366\n",
      "Iteration 689, the loss is 5.1749341528804065, parameters k is 14.802970427760963 and b is -70.35094525745497\n",
      "Iteration 690, the loss is 5.174917193995442, parameters k is 14.803037720251082 and b is -70.35051047484627\n",
      "Iteration 691, the loss is 5.174899182643796, parameters k is 14.802804222227365 and b is -70.35011521792927\n",
      "Iteration 692, the loss is 5.174880505242683, parameters k is 14.802871514717484 and b is -70.34968043532058\n",
      "Iteration 693, the loss is 5.174864212407189, parameters k is 14.802638016693768 and b is -70.34928517840358\n",
      "Iteration 694, the loss is 5.174844855987588, parameters k is 14.802705309183887 and b is -70.34885039579488\n",
      "Iteration 695, the loss is 5.174828202672933, parameters k is 14.802772601674006 and b is -70.34841561318619\n",
      "Iteration 696, the loss is 5.17480988575097, parameters k is 14.80253910365029 and b is -70.34802035626919\n",
      "Iteration 697, the loss is 5.174791513920171, parameters k is 14.802606396140408 and b is -70.3475855736605\n",
      "Iteration 698, the loss is 5.174774915514362, parameters k is 14.802372898116692 and b is -70.3471903167435\n",
      "Iteration 699, the loss is 5.174755559094764, parameters k is 14.802440190606811 and b is -70.3467555341348\n",
      "Iteration 700, the loss is 5.174739211350423, parameters k is 14.80250748309693 and b is -70.34632075152611\n",
      "Iteration 701, the loss is 5.174720588858155, parameters k is 14.802273985073214 and b is -70.3459254946091\n",
      "Iteration 702, the loss is 5.174702522597665, parameters k is 14.802341277563333 and b is -70.34549071200041\n",
      "Iteration 703, the loss is 5.174685618621551, parameters k is 14.802107779539616 and b is -70.34509545508341\n",
      "Iteration 704, the loss is 5.174666262201941, parameters k is 14.802175072029735 and b is -70.34466067247472\n",
      "Iteration 705, the loss is 5.174650220027908, parameters k is 14.802242364519854 and b is -70.34422588986602\n",
      "Iteration 706, the loss is 5.1746312919653334, parameters k is 14.802008866496138 and b is -70.34383063294902\n",
      "Iteration 707, the loss is 5.174613531275157, parameters k is 14.802076158986257 and b is -70.34339585034033\n",
      "Iteration 708, the loss is 5.174596321728724, parameters k is 14.80184266096254 and b is -70.34300059342333\n",
      "Iteration 709, the loss is 5.17457696530912, parameters k is 14.80190995345266 and b is -70.34256581081463\n",
      "Iteration 710, the loss is 5.1745612287054055, parameters k is 14.801977245942778 and b is -70.34213102820594\n",
      "Iteration 711, the loss is 5.17454199507251, parameters k is 14.801743747919062 and b is -70.34173577128894\n",
      "Iteration 712, the loss is 5.174524539952648, parameters k is 14.80181104040918 and b is -70.34130098868025\n",
      "Iteration 713, the loss is 5.174507024835911, parameters k is 14.801577542385465 and b is -70.34090573176324\n",
      "Iteration 714, the loss is 5.174487851199888, parameters k is 14.801644834875583 and b is -70.34047094915455\n",
      "Iteration 715, the loss is 5.174472054599302, parameters k is 14.801411336851867 and b is -70.34007569223755\n",
      "Iteration 716, the loss is 5.174452698179693, parameters k is 14.801478629341986 and b is -70.33964090962886\n",
      "Iteration 717, the loss is 5.174435548630133, parameters k is 14.801545921832105 and b is -70.33920612702016\n",
      "Iteration 718, the loss is 5.174417727943084, parameters k is 14.801312423808389 and b is -70.33881087010316\n",
      "Iteration 719, the loss is 5.174398859877387, parameters k is 14.801379716298507 and b is -70.33837608749447\n",
      "Iteration 720, the loss is 5.174382757706482, parameters k is 14.801146218274791 and b is -70.33798083057746\n",
      "Iteration 721, the loss is 5.174363401286881, parameters k is 14.80121351076491 and b is -70.33754604796877\n",
      "Iteration 722, the loss is 5.174346557307626, parameters k is 14.801280803255029 and b is -70.33711126536008\n",
      "Iteration 723, the loss is 5.174328431050266, parameters k is 14.801047305231313 and b is -70.33671600844308\n",
      "Iteration 724, the loss is 5.174309868554879, parameters k is 14.801114597721432 and b is -70.33628122583438\n",
      "Iteration 725, the loss is 5.174293460813656, parameters k is 14.800881099697715 and b is -70.33588596891738\n",
      "Iteration 726, the loss is 5.174274104394055, parameters k is 14.800948392187834 and b is -70.33545118630869\n",
      "Iteration 727, the loss is 5.1742575659851155, parameters k is 14.801015684677953 and b is -70.3350164037\n",
      "Iteration 728, the loss is 5.174239134157451, parameters k is 14.800782186654237 and b is -70.334621146783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 729, the loss is 5.1742208772323615, parameters k is 14.800849479144356 and b is -70.3341863641743\n",
      "Iteration 730, the loss is 5.1742041639208445, parameters k is 14.80061598112064 and b is -70.3337911072573\n",
      "Iteration 731, the loss is 5.174184807501239, parameters k is 14.800683273610758 and b is -70.3333563246486\n",
      "Iteration 732, the loss is 5.174168574662611, parameters k is 14.800750566100877 and b is -70.33292154203991\n",
      "Iteration 733, the loss is 5.174149837264626, parameters k is 14.800517068077161 and b is -70.33252628512291\n",
      "Iteration 734, the loss is 5.174131885909852, parameters k is 14.80058436056728 and b is -70.33209150251422\n",
      "Iteration 735, the loss is 5.174114867028017, parameters k is 14.800350862543564 and b is -70.33169624559721\n",
      "Iteration 736, the loss is 5.17409551060842, parameters k is 14.800418155033682 and b is -70.33126146298852\n",
      "Iteration 737, the loss is 5.174079583340101, parameters k is 14.800485447523801 and b is -70.33082668037983\n",
      "Iteration 738, the loss is 5.174060540371803, parameters k is 14.800251949500085 and b is -70.33043142346283\n",
      "Iteration 739, the loss is 5.174042894587348, parameters k is 14.800319241990204 and b is -70.32999664085413\n",
      "Iteration 740, the loss is 5.174025570135199, parameters k is 14.800085743966488 and b is -70.32960138393713\n",
      "Iteration 741, the loss is 5.174006213715597, parameters k is 14.800153036456607 and b is -70.32916660132844\n",
      "Iteration 742, the loss is 5.173990592017585, parameters k is 14.800220328946725 and b is -70.32873181871975\n",
      "Iteration 743, the loss is 5.173971243478987, parameters k is 14.79998683092301 and b is -70.32833656180274\n",
      "Iteration 744, the loss is 5.173953903264832, parameters k is 14.800054123413128 and b is -70.32790177919405\n",
      "Iteration 745, the loss is 5.173936273242378, parameters k is 14.799820625389412 and b is -70.32750652227705\n",
      "Iteration 746, the loss is 5.173917214512081, parameters k is 14.79988791787953 and b is -70.32707173966836\n",
      "Iteration 747, the loss is 5.17390130300577, parameters k is 14.799654419855814 and b is -70.32667648275135\n",
      "Iteration 748, the loss is 5.173881946586172, parameters k is 14.799721712345933 and b is -70.32624170014266\n",
      "Iteration 749, the loss is 5.173864911942331, parameters k is 14.799789004836052 and b is -70.32580691753397\n",
      "Iteration 750, the loss is 5.17384697634956, parameters k is 14.799555506812336 and b is -70.32541166061696\n",
      "Iteration 751, the loss is 5.173828223189572, parameters k is 14.799622799302455 and b is -70.32497687800827\n",
      "Iteration 752, the loss is 5.173812006112948, parameters k is 14.799389301278739 and b is -70.32458162109127\n",
      "Iteration 753, the loss is 5.173792649693342, parameters k is 14.799456593768857 and b is -70.32414683848258\n",
      "Iteration 754, the loss is 5.173775920619821, parameters k is 14.799523886258976 and b is -70.32371205587388\n",
      "Iteration 755, the loss is 5.17375767945674, parameters k is 14.79929038823526 and b is -70.32331679895688\n",
      "Iteration 756, the loss is 5.173739231867066, parameters k is 14.799357680725379 and b is -70.32288201634819\n",
      "Iteration 757, the loss is 5.173722709220138, parameters k is 14.799124182701663 and b is -70.32248675943119\n",
      "Iteration 758, the loss is 5.173703352800528, parameters k is 14.799191475191781 and b is -70.3220519768225\n",
      "Iteration 759, the loss is 5.173686929297305, parameters k is 14.7992587676819 and b is -70.3216171942138\n",
      "Iteration 760, the loss is 5.173668382563919, parameters k is 14.799025269658184 and b is -70.3212219372968\n",
      "Iteration 761, the loss is 5.173650240544553, parameters k is 14.799092562148303 and b is -70.3207871546881\n",
      "Iteration 762, the loss is 5.173633412327313, parameters k is 14.798859064124587 and b is -70.3203918977711\n",
      "Iteration 763, the loss is 5.173614055907702, parameters k is 14.798926356614706 and b is -70.31995711516241\n",
      "Iteration 764, the loss is 5.173597937974802, parameters k is 14.798993649104824 and b is -70.31952233255372\n",
      "Iteration 765, the loss is 5.173579085671101, parameters k is 14.798760151081108 and b is -70.31912707563671\n",
      "Iteration 766, the loss is 5.173561249222049, parameters k is 14.798827443571227 and b is -70.31869229302802\n",
      "Iteration 767, the loss is 5.173544115434495, parameters k is 14.79859394554751 and b is -70.31829703611102\n",
      "Iteration 768, the loss is 5.173524759014891, parameters k is 14.79866123803763 and b is -70.31786225350233\n",
      "Iteration 769, the loss is 5.173508946652288, parameters k is 14.798728530527749 and b is -70.31742747089363\n",
      "Iteration 770, the loss is 5.173489788778279, parameters k is 14.798495032504032 and b is -70.31703221397663\n",
      "Iteration 771, the loss is 5.173472257899535, parameters k is 14.798562324994151 and b is -70.31659743136794\n",
      "Iteration 772, the loss is 5.17345481854167, parameters k is 14.798328826970435 and b is -70.31620217445094\n",
      "Iteration 773, the loss is 5.173435569146777, parameters k is 14.798396119460554 and b is -70.31576739184224\n",
      "Iteration 774, the loss is 5.173419848305068, parameters k is 14.798162621436838 and b is -70.31537213492524\n",
      "Iteration 775, the loss is 5.173400491885455, parameters k is 14.798229913926956 and b is -70.31493735231655\n",
      "Iteration 776, the loss is 5.173383266577021, parameters k is 14.798297206417075 and b is -70.31450256970786\n",
      "Iteration 777, the loss is 5.173365521648851, parameters k is 14.798063708393359 and b is -70.31410731279085\n",
      "Iteration 778, the loss is 5.173346577824273, parameters k is 14.798131000883478 and b is -70.31367253018216\n",
      "Iteration 779, the loss is 5.173330551412243, parameters k is 14.797897502859762 and b is -70.31327727326516\n",
      "Iteration 780, the loss is 5.1733111949926425, parameters k is 14.79796479534988 and b is -70.31284249065646\n",
      "Iteration 781, the loss is 5.17329427525452, parameters k is 14.79803208784 and b is -70.31240770804777\n",
      "Iteration 782, the loss is 5.173276224756032, parameters k is 14.797798589816283 and b is -70.31201245113077\n",
      "Iteration 783, the loss is 5.173257586501761, parameters k is 14.797865882306402 and b is -70.31157766852208\n",
      "Iteration 784, the loss is 5.173241254519423, parameters k is 14.797632384282686 and b is -70.31118241160507\n",
      "Iteration 785, the loss is 5.173221898099815, parameters k is 14.797699676772805 and b is -70.31074762899638\n",
      "Iteration 786, the loss is 5.173205283932005, parameters k is 14.797766969262923 and b is -70.31031284638769\n",
      "Iteration 787, the loss is 5.17318692786321, parameters k is 14.797533471239207 and b is -70.30991758947069\n",
      "Iteration 788, the loss is 5.173168595179249, parameters k is 14.797600763729326 and b is -70.309482806862\n",
      "Iteration 789, the loss is 5.173151957626606, parameters k is 14.79736726570561 and b is -70.30908754994499\n",
      "Iteration 790, the loss is 5.173132601207002, parameters k is 14.797434558195729 and b is -70.3086527673363\n",
      "Iteration 791, the loss is 5.173116292609488, parameters k is 14.797501850685848 and b is -70.3082179847276\n",
      "Iteration 792, the loss is 5.1730976309703935, parameters k is 14.797268352662131 and b is -70.3078227278106\n",
      "Iteration 793, the loss is 5.173079603856744, parameters k is 14.79733564515225 and b is -70.30738794520191\n",
      "Iteration 794, the loss is 5.173062660733782, parameters k is 14.797102147128534 and b is -70.30699268828491\n",
      "Iteration 795, the loss is 5.173043304314178, parameters k is 14.797169439618653 and b is -70.30655790567621\n",
      "Iteration 796, the loss is 5.17302730128699, parameters k is 14.797236732108772 and b is -70.30612312306752\n",
      "Iteration 797, the loss is 5.173008334077572, parameters k is 14.797003234085055 and b is -70.30572786615052\n",
      "Iteration 798, the loss is 5.172990612534236, parameters k is 14.797070526575174 and b is -70.30529308354183\n",
      "Iteration 799, the loss is 5.172973363840962, parameters k is 14.796837028551458 and b is -70.30489782662482\n",
      "Iteration 800, the loss is 5.172954007421359, parameters k is 14.796904321041577 and b is -70.30446304401613\n",
      "Iteration 801, the loss is 5.17293830996448, parameters k is 14.796971613531696 and b is -70.30402826140744\n",
      "Iteration 802, the loss is 5.1729190371847595, parameters k is 14.79673811550798 and b is -70.30363300449044\n",
      "Iteration 803, the loss is 5.172901621211728, parameters k is 14.796805407998098 and b is -70.30319822188174\n",
      "Iteration 804, the loss is 5.172884066948147, parameters k is 14.796571909974382 and b is -70.30280296496474\n",
      "Iteration 805, the loss is 5.172864932458974, parameters k is 14.796639202464501 and b is -70.30236818235605\n",
      "Iteration 806, the loss is 5.172849096711535, parameters k is 14.796405704440785 and b is -70.30197292543905\n",
      "Iteration 807, the loss is 5.172829740291935, parameters k is 14.796472996930904 and b is -70.30153814283035\n",
      "Iteration 808, the loss is 5.172812629889218, parameters k is 14.796540289421023 and b is -70.30110336022166\n",
      "Iteration 809, the loss is 5.172794770055322, parameters k is 14.796306791397306 and b is -70.30070810330466\n",
      "Iteration 810, the loss is 5.172775941136459, parameters k is 14.796374083887425 and b is -70.30027332069596\n",
      "Iteration 811, the loss is 5.172759799818716, parameters k is 14.796140585863709 and b is -70.29987806377896\n",
      "Iteration 812, the loss is 5.172740443399108, parameters k is 14.796207878353828 and b is -70.29944328117027\n",
      "Iteration 813, the loss is 5.1727236385666995, parameters k is 14.796275170843947 and b is -70.29900849856158\n",
      "Iteration 814, the loss is 5.172705473162506, parameters k is 14.79604167282023 and b is -70.29861324164457\n",
      "Iteration 815, the loss is 5.17268694981395, parameters k is 14.79610896531035 and b is -70.29817845903588\n",
      "Iteration 816, the loss is 5.1726705029258975, parameters k is 14.795875467286633 and b is -70.29778320211888\n",
      "Iteration 817, the loss is 5.172651146506293, parameters k is 14.795942759776752 and b is -70.29734841951019\n",
      "Iteration 818, the loss is 5.172634647244194, parameters k is 14.79601005226687 and b is -70.2969136369015\n",
      "Iteration 819, the loss is 5.17261617626968, parameters k is 14.795776554243155 and b is -70.29651837998449\n",
      "Iteration 820, the loss is 5.172597958491446, parameters k is 14.795843846733273 and b is -70.2960835973758\n",
      "Iteration 821, the loss is 5.172581206033075, parameters k is 14.795610348709557 and b is -70.2956883404588\n",
      "Iteration 822, the loss is 5.172561849613474, parameters k is 14.795677641199676 and b is -70.2952535578501\n",
      "Iteration 823, the loss is 5.172545655921684, parameters k is 14.795744933689795 and b is -70.29481877524141\n",
      "Iteration 824, the loss is 5.172526879376867, parameters k is 14.795511435666079 and b is -70.29442351832441\n",
      "Iteration 825, the loss is 5.172508967168935, parameters k is 14.795578728156197 and b is -70.29398873571571\n",
      "Iteration 826, the loss is 5.172491909140263, parameters k is 14.795345230132481 and b is -70.29359347879871\n",
      "Iteration 827, the loss is 5.1724725527206505, parameters k is 14.7954125226226 and b is -70.29315869619002\n",
      "Iteration 828, the loss is 5.172456664599176, parameters k is 14.795479815112719 and b is -70.29272391358133\n",
      "Iteration 829, the loss is 5.172437582484044, parameters k is 14.795246317089003 and b is -70.29232865666432\n",
      "Iteration 830, the loss is 5.172419975846417, parameters k is 14.795313609579122 and b is -70.29189387405563\n",
      "Iteration 831, the loss is 5.172402612247434, parameters k is 14.795080111555405 and b is -70.29149861713863\n",
      "Iteration 832, the loss is 5.172383287093673, parameters k is 14.795147404045524 and b is -70.29106383452994\n",
      "Iteration 833, the loss is 5.172367642010832, parameters k is 14.794913906021808 and b is -70.29066857761293\n",
      "Iteration 834, the loss is 5.172348285591226, parameters k is 14.794981198511927 and b is -70.29023379500424\n",
      "Iteration 835, the loss is 5.172330984523913, parameters k is 14.795048491002046 and b is -70.28979901239555\n",
      "Iteration 836, the loss is 5.17231331535462, parameters k is 14.79481499297833 and b is -70.28940375547855\n",
      "Iteration 837, the loss is 5.172294295771161, parameters k is 14.794882285468448 and b is -70.28896897286985\n",
      "Iteration 838, the loss is 5.172278345118014, parameters k is 14.794648787444732 and b is -70.28857371595285\n",
      "Iteration 839, the loss is 5.172258988698406, parameters k is 14.794716079934851 and b is -70.28813893334416\n",
      "Iteration 840, the loss is 5.172241993201409, parameters k is 14.79478337242497 and b is -70.28770415073546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 841, the loss is 5.172224018461801, parameters k is 14.794549874401254 and b is -70.28730889381846\n",
      "Iteration 842, the loss is 5.1722053044486485, parameters k is 14.794617166891372 and b is -70.28687411120977\n",
      "Iteration 843, the loss is 5.172189048225188, parameters k is 14.794383668867656 and b is -70.28647885429277\n",
      "Iteration 844, the loss is 5.17216969180559, parameters k is 14.794450961357775 and b is -70.28604407168407\n",
      "Iteration 845, the loss is 5.17215300187889, parameters k is 14.794518253847894 and b is -70.28560928907538\n",
      "Iteration 846, the loss is 5.1721347215689795, parameters k is 14.794284755824178 and b is -70.28521403215838\n",
      "Iteration 847, the loss is 5.172116313126142, parameters k is 14.794352048314297 and b is -70.28477924954969\n",
      "Iteration 848, the loss is 5.172099751332372, parameters k is 14.79411855029058 and b is -70.28438399263268\n",
      "Iteration 849, the loss is 5.172080394912761, parameters k is 14.7941858427807 and b is -70.28394921002399\n",
      "Iteration 850, the loss is 5.172064010556387, parameters k is 14.794253135270818 and b is -70.2835144274153\n",
      "Iteration 851, the loss is 5.172045424676157, parameters k is 14.794019637247102 and b is -70.2831191704983\n",
      "Iteration 852, the loss is 5.172027321803634, parameters k is 14.79408692973722 and b is -70.2826843878896\n",
      "Iteration 853, the loss is 5.172010454439557, parameters k is 14.793853431713504 and b is -70.2822891309726\n",
      "Iteration 854, the loss is 5.17199109801994, parameters k is 14.793920724203623 and b is -70.28185434836391\n",
      "Iteration 855, the loss is 5.171975019233875, parameters k is 14.793988016693742 and b is -70.28141956575521\n",
      "Iteration 856, the loss is 5.171956127783342, parameters k is 14.793754518670026 and b is -70.28102430883821\n",
      "Iteration 857, the loss is 5.171938330481124, parameters k is 14.793821811160145 and b is -70.28058952622952\n",
      "Iteration 858, the loss is 5.171921157546733, parameters k is 14.793588313136429 and b is -70.28019426931252\n",
      "Iteration 859, the loss is 5.171901801127124, parameters k is 14.793655605626547 and b is -70.27975948670382\n",
      "Iteration 860, the loss is 5.1718860279113645, parameters k is 14.793722898116666 and b is -70.27932470409513\n",
      "Iteration 861, the loss is 5.171866830890524, parameters k is 14.79348940009295 and b is -70.27892944717813\n",
      "Iteration 862, the loss is 5.1718493391586104, parameters k is 14.793556692583069 and b is -70.27849466456944\n",
      "Iteration 863, the loss is 5.1718318606539135, parameters k is 14.793323194559353 and b is -70.27809940765243\n",
      "Iteration 864, the loss is 5.17181265040586, parameters k is 14.793390487049471 and b is -70.27766462504374\n",
      "Iteration 865, the loss is 5.171796890417303, parameters k is 14.793156989025755 and b is -70.27726936812674\n",
      "Iteration 866, the loss is 5.171777533997696, parameters k is 14.793224281515874 and b is -70.27683458551805\n",
      "Iteration 867, the loss is 5.1717603478360985, parameters k is 14.793291574005993 and b is -70.27639980290935\n",
      "Iteration 868, the loss is 5.171742563761092, parameters k is 14.793058075982277 and b is -70.27600454599235\n",
      "Iteration 869, the loss is 5.171723659083348, parameters k is 14.793125368472396 and b is -70.27556976338366\n",
      "Iteration 870, the loss is 5.171707593524485, parameters k is 14.79289187044868 and b is -70.27517450646666\n",
      "Iteration 871, the loss is 5.171688237104879, parameters k is 14.792959162938798 and b is -70.27473972385796\n",
      "Iteration 872, the loss is 5.171671356513594, parameters k is 14.793026455428917 and b is -70.27430494124927\n",
      "Iteration 873, the loss is 5.171653266868274, parameters k is 14.7927929574052 and b is -70.27390968433227\n",
      "Iteration 874, the loss is 5.171634667760842, parameters k is 14.79286024989532 and b is -70.27347490172357\n",
      "Iteration 875, the loss is 5.171618296631665, parameters k is 14.792626751871603 and b is -70.27307964480657\n",
      "Iteration 876, the loss is 5.171598940212065, parameters k is 14.792694044361722 and b is -70.27264486219788\n",
      "Iteration 877, the loss is 5.17158236519109, parameters k is 14.792761336851841 and b is -70.27221007958919\n",
      "Iteration 878, the loss is 5.17156396997545, parameters k is 14.792527838828125 and b is -70.27181482267218\n",
      "Iteration 879, the loss is 5.171545676438327, parameters k is 14.792595131318244 and b is -70.27138004006349\n",
      "Iteration 880, the loss is 5.171528999738846, parameters k is 14.792361633294528 and b is -70.27098478314649\n",
      "Iteration 881, the loss is 5.171509643319237, parameters k is 14.792428925784646 and b is -70.2705500005378\n",
      "Iteration 882, the loss is 5.171493373868572, parameters k is 14.792496218274765 and b is -70.2701152179291\n",
      "Iteration 883, the loss is 5.171474673082631, parameters k is 14.792262720251049 and b is -70.2697199610121\n",
      "Iteration 884, the loss is 5.171456685115821, parameters k is 14.792330012741168 and b is -70.26928517840341\n",
      "Iteration 885, the loss is 5.171439702846017, parameters k is 14.792096514717452 and b is -70.2688899214864\n",
      "Iteration 886, the loss is 5.17142034642642, parameters k is 14.79216380720757 and b is -70.26845513887771\n",
      "Iteration 887, the loss is 5.171404382546063, parameters k is 14.79223109969769 and b is -70.26802035626902\n",
      "Iteration 888, the loss is 5.171385376189813, parameters k is 14.791997601673973 and b is -70.26762509935202\n",
      "Iteration 889, the loss is 5.1713676937933135, parameters k is 14.792064894164092 and b is -70.26719031674332\n",
      "Iteration 890, the loss is 5.1713504059532, parameters k is 14.791831396140376 and b is -70.26679505982632\n",
      "Iteration 891, the loss is 5.171331049533596, parameters k is 14.791898688630495 and b is -70.26636027721763\n",
      "Iteration 892, the loss is 5.1713153912235565, parameters k is 14.791965981120613 and b is -70.26592549460894\n",
      "Iteration 893, the loss is 5.171296079296994, parameters k is 14.791732483096897 and b is -70.26553023769193\n",
      "Iteration 894, the loss is 5.171278702470799, parameters k is 14.791799775587016 and b is -70.26509545508324\n",
      "Iteration 895, the loss is 5.171261109060384, parameters k is 14.7915662775633 and b is -70.26470019816624\n",
      "Iteration 896, the loss is 5.171242013718046, parameters k is 14.791633570053419 and b is -70.26426541555755\n",
      "Iteration 897, the loss is 5.171226138823778, parameters k is 14.791400072029703 and b is -70.26387015864054\n",
      "Iteration 898, the loss is 5.171206782404172, parameters k is 14.791467364519821 and b is -70.26343537603185\n",
      "Iteration 899, the loss is 5.171189711148296, parameters k is 14.79153465700994 and b is -70.26300059342316\n",
      "Iteration 900, the loss is 5.1711718121675645, parameters k is 14.791301158986224 and b is -70.26260533650616\n",
      "Iteration 901, the loss is 5.17115302239554, parameters k is 14.791368451476343 and b is -70.26217055389746\n",
      "Iteration 902, the loss is 5.171136841930954, parameters k is 14.791134953452627 and b is -70.26177529698046\n",
      "Iteration 903, the loss is 5.171117485511345, parameters k is 14.791202245942745 and b is -70.26134051437177\n",
      "Iteration 904, the loss is 5.171100719825784, parameters k is 14.791269538432864 and b is -70.26090573176307\n",
      "Iteration 905, the loss is 5.171082515274742, parameters k is 14.791036040409148 and b is -70.26051047484607\n",
      "Iteration 906, the loss is 5.171064031073027, parameters k is 14.791103332899267 and b is -70.26007569223738\n",
      "Iteration 907, the loss is 5.171047545038138, parameters k is 14.79086983487555 and b is -70.25968043532038\n",
      "Iteration 908, the loss is 5.171028188618532, parameters k is 14.79093712736567 and b is -70.25924565271168\n",
      "Iteration 909, the loss is 5.171011728503274, parameters k is 14.791004419855788 and b is -70.25881087010299\n",
      "Iteration 910, the loss is 5.170993218381925, parameters k is 14.790770921832072 and b is -70.25841561318599\n",
      "Iteration 911, the loss is 5.170975039750518, parameters k is 14.790838214322191 and b is -70.2579808305773\n",
      "Iteration 912, the loss is 5.170958248145317, parameters k is 14.790604716298475 and b is -70.2575855736603\n",
      "Iteration 913, the loss is 5.170938891725712, parameters k is 14.790672008788594 and b is -70.2571507910516\n",
      "Iteration 914, the loss is 5.17092273718076, parameters k is 14.790739301278713 and b is -70.25671600844291\n",
      "Iteration 915, the loss is 5.170903921489105, parameters k is 14.790505803254996 and b is -70.2563207515259\n",
      "Iteration 916, the loss is 5.170886048428015, parameters k is 14.790573095745115 and b is -70.25588596891721\n",
      "Iteration 917, the loss is 5.170868951252494, parameters k is 14.790339597721399 and b is -70.25549071200021\n",
      "Iteration 918, the loss is 5.170849594832896, parameters k is 14.790406890211518 and b is -70.25505592939152\n",
      "Iteration 919, the loss is 5.170833745858255, parameters k is 14.790474182701637 and b is -70.25462114678282\n",
      "Iteration 920, the loss is 5.170814624596284, parameters k is 14.79024068467792 and b is -70.25422588986582\n",
      "Iteration 921, the loss is 5.170797057105496, parameters k is 14.79030797716804 and b is -70.25379110725713\n",
      "Iteration 922, the loss is 5.170779654359678, parameters k is 14.790074479144323 and b is -70.25339585034013\n",
      "Iteration 923, the loss is 5.170760368352744, parameters k is 14.790141771634442 and b is -70.25296106773143\n",
      "Iteration 924, the loss is 5.170744684123069, parameters k is 14.789908273610726 and b is -70.25256581081443\n",
      "Iteration 925, the loss is 5.170725327703467, parameters k is 14.789975566100845 and b is -70.25213102820574\n",
      "Iteration 926, the loss is 5.17070806578299, parameters k is 14.790042858590963 and b is -70.25169624559705\n",
      "Iteration 927, the loss is 5.17069035746686, parameters k is 14.789809360567247 and b is -70.25130098868004\n",
      "Iteration 928, the loss is 5.170671377030234, parameters k is 14.789876653057366 and b is -70.25086620607135\n",
      "Iteration 929, the loss is 5.170655387230245, parameters k is 14.78964315503365 and b is -70.25047094915435\n",
      "Iteration 930, the loss is 5.170636030810643, parameters k is 14.789710447523769 and b is -70.25003616654566\n",
      "Iteration 931, the loss is 5.170619074460479, parameters k is 14.789777740013887 and b is -70.24960138393696\n",
      "Iteration 932, the loss is 5.170601060574034, parameters k is 14.789544241990171 and b is -70.24920612701996\n",
      "Iteration 933, the loss is 5.170582385707728, parameters k is 14.78961153448029 and b is -70.24877134441127\n",
      "Iteration 934, the loss is 5.170566090337431, parameters k is 14.789378036456574 and b is -70.24837608749426\n",
      "Iteration 935, the loss is 5.170546733917826, parameters k is 14.789445328946693 and b is -70.24794130488557\n",
      "Iteration 936, the loss is 5.170530083137973, parameters k is 14.789512621436812 and b is -70.24750652227688\n",
      "Iteration 937, the loss is 5.170511763681216, parameters k is 14.789279123413095 and b is -70.24711126535988\n",
      "Iteration 938, the loss is 5.170493394385215, parameters k is 14.789346415903214 and b is -70.24667648275118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 939, the loss is 5.1704767934446085, parameters k is 14.789112917879498 and b is -70.24628122583418\n",
      "Iteration 940, the loss is 5.170457437025009, parameters k is 14.789180210369617 and b is -70.24584644322549\n",
      "Iteration 941, the loss is 5.17044109181546, parameters k is 14.789247502859736 and b is -70.2454116606168\n",
      "Iteration 942, the loss is 5.170422466788391, parameters k is 14.78901400483602 and b is -70.2450164036998\n",
      "Iteration 943, the loss is 5.170404403062706, parameters k is 14.789081297326138 and b is -70.2445816210911\n",
      "Iteration 944, the loss is 5.170387496551786, parameters k is 14.788847799302422 and b is -70.2441863641741\n",
      "Iteration 945, the loss is 5.170368140132186, parameters k is 14.788915091792541 and b is -70.2437515815654\n",
      "Iteration 946, the loss is 5.170352100492957, parameters k is 14.78898238428266 and b is -70.24331679895671\n",
      "Iteration 947, the loss is 5.170333169895576, parameters k is 14.788748886258944 and b is -70.24292154203971\n",
      "Iteration 948, the loss is 5.170315411740201, parameters k is 14.788816178749062 and b is -70.24248675943102\n",
      "Iteration 949, the loss is 5.170298199658971, parameters k is 14.788582680725346 and b is -70.24209150251401\n",
      "Iteration 950, the loss is 5.170278843239363, parameters k is 14.788649973215465 and b is -70.24165671990532\n",
      "Iteration 951, the loss is 5.170263109170438, parameters k is 14.788717265705584 and b is -70.24122193729663\n",
      "Iteration 952, the loss is 5.170243873002763, parameters k is 14.788483767681868 and b is -70.24082668037963\n",
      "Iteration 953, the loss is 5.170226420417693, parameters k is 14.788551060171987 and b is -70.24039189777093\n",
      "Iteration 954, the loss is 5.170208902766148, parameters k is 14.78831756214827 and b is -70.23999664085393\n",
      "Iteration 955, the loss is 5.17018973166494, parameters k is 14.78838485463839 and b is -70.23956185824524\n",
      "Iteration 956, the loss is 5.170173932529544, parameters k is 14.788151356614673 and b is -70.23916660132824\n",
      "Iteration 957, the loss is 5.170154576109935, parameters k is 14.788218649104792 and b is -70.23873181871954\n",
      "Iteration 958, the loss is 5.1701374290951785, parameters k is 14.78828594159491 and b is -70.23829703611085\n",
      "Iteration 959, the loss is 5.170119605873331, parameters k is 14.788052443571194 and b is -70.23790177919385\n",
      "Iteration 960, the loss is 5.170100740342427, parameters k is 14.788119736061313 and b is -70.23746699658516\n",
      "Iteration 961, the loss is 5.170084635636725, parameters k is 14.787886238037597 and b is -70.23707173966815\n",
      "Iteration 962, the loss is 5.17006527921712, parameters k is 14.787953530527716 and b is -70.23663695705946\n",
      "Iteration 963, the loss is 5.1700484377726665, parameters k is 14.788020823017835 and b is -70.23620217445077\n",
      "Iteration 964, the loss is 5.17003030898051, parameters k is 14.787787324994119 and b is -70.23580691753376\n",
      "Iteration 965, the loss is 5.170011749019919, parameters k is 14.787854617484237 and b is -70.23537213492507\n",
      "Iteration 966, the loss is 5.169995338743904, parameters k is 14.787621119460521 and b is -70.23497687800807\n",
      "Iteration 967, the loss is 5.169975982324299, parameters k is 14.78768841195064 and b is -70.23454209539938\n",
      "Iteration 968, the loss is 5.169959446450164, parameters k is 14.787755704440759 and b is -70.23410731279068\n",
      "Iteration 969, the loss is 5.169941012087685, parameters k is 14.787522206417043 and b is -70.23371205587368\n",
      "Iteration 970, the loss is 5.169922757697408, parameters k is 14.787589498907161 and b is -70.23327727326499\n",
      "Iteration 971, the loss is 5.169906041851077, parameters k is 14.787356000883445 and b is -70.23288201634799\n",
      "Iteration 972, the loss is 5.1698866854314725, parameters k is 14.787423293373564 and b is -70.2324472337393\n",
      "Iteration 973, the loss is 5.16987045512765, parameters k is 14.787490585863683 and b is -70.2320124511306\n",
      "Iteration 974, the loss is 5.169851715194866, parameters k is 14.787257087839967 and b is -70.2316171942136\n",
      "Iteration 975, the loss is 5.169833766374894, parameters k is 14.787324380330086 and b is -70.2311824116049\n",
      "Iteration 976, the loss is 5.169816744958265, parameters k is 14.78709088230637 and b is -70.2307871546879\n",
      "Iteration 977, the loss is 5.169797388538658, parameters k is 14.787158174796488 and b is -70.23035237207921\n",
      "Iteration 978, the loss is 5.1697814638051405, parameters k is 14.787225467286607 and b is -70.22991758947052\n",
      "Iteration 979, the loss is 5.169762418302047, parameters k is 14.78699196926289 and b is -70.22952233255351\n",
      "Iteration 980, the loss is 5.16974477505239, parameters k is 14.78705926175301 and b is -70.22908754994482\n",
      "Iteration 981, the loss is 5.169727448065436, parameters k is 14.786825763729293 and b is -70.22869229302782\n",
      "Iteration 982, the loss is 5.169708091645838, parameters k is 14.786893056219412 and b is -70.22825751041913\n",
      "Iteration 983, the loss is 5.169692472482634, parameters k is 14.786960348709531 and b is -70.22782272781043\n",
      "Iteration 984, the loss is 5.169673121409232, parameters k is 14.786726850685815 and b is -70.22742747089343\n",
      "Iteration 985, the loss is 5.16965578372988, parameters k is 14.786794143175934 and b is -70.22699268828474\n",
      "Iteration 986, the loss is 5.169638151172622, parameters k is 14.786560645152218 and b is -70.22659743136774\n",
      "Iteration 987, the loss is 5.169619094977123, parameters k is 14.786627937642336 and b is -70.22616264875904\n",
      "Iteration 988, the loss is 5.169603180936019, parameters k is 14.78639443961862 and b is -70.22576739184204\n",
      "Iteration 989, the loss is 5.169583824516409, parameters k is 14.786461732108739 and b is -70.22533260923335\n",
      "Iteration 990, the loss is 5.16956679240737, parameters k is 14.786529024598858 and b is -70.22489782662466\n",
      "Iteration 991, the loss is 5.169548854279805, parameters k is 14.786295526575142 and b is -70.22450256970765\n",
      "Iteration 992, the loss is 5.16953010365461, parameters k is 14.78636281906526 and b is -70.22406778709896\n",
      "Iteration 993, the loss is 5.169513884043199, parameters k is 14.786129321041544 and b is -70.22367253018196\n",
      "Iteration 994, the loss is 5.1694945276235895, parameters k is 14.786196613531663 and b is -70.22323774757326\n",
      "Iteration 995, the loss is 5.16947780108486, parameters k is 14.786263906021782 and b is -70.22280296496457\n",
      "Iteration 996, the loss is 5.169459557386982, parameters k is 14.786030407998066 and b is -70.22240770804757\n",
      "Iteration 997, the loss is 5.16944111233211, parameters k is 14.786097700488185 and b is -70.22197292543888\n",
      "Iteration 998, the loss is 5.169424587150375, parameters k is 14.785864202464468 and b is -70.22157766852187\n",
      "Iteration 999, the loss is 5.169405230730773, parameters k is 14.785931494954587 and b is -70.22114288591318\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "iteration_num = 1000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110cab828>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10eafcd68>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
