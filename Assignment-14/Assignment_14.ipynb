{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本周只有一个代码实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <评阅点>     \n",
    "1. 完成代码并无bug  50‘   \n",
    "2. 代码有部分bug   40'   \n",
    "3. 代码有重大bug   30‘\n",
    "4. 代码不完整  20'   \n",
    "5. 其余 0‘   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码课用来加载 预先训练好的模型,你需要只需要修改模型的存放路径即可（第二行代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步 使用以下链接下载相应预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "weights = torch.load('../lecture13/small_ft.pkl')\n",
    "medium_config = GPT2Config(n_embd = 768,n_layer = 12, n_head = 12)\n",
    "model = GPT2LMHeadModel(medium_config)\n",
    "\n",
    "weights['lm_head.weight'] = weights['lm_head.decoder.weight']\n",
    "weights.pop('lm_head.decoder.weight',None)\n",
    "\n",
    "model.load_state_dict(weights)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你需要写一个推理函数，这个函数接收一个英文句子为输入，输出一个回应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试语句 \n",
    "一轮对话   \n",
    "1. Does money buy happiness ?   \n",
    "2. What is the best way to buy happiness?   \n",
    "\n",
    "一轮对话   \n",
    "1. what is the meaning of a godd life ?   \n",
    "2. How to be a good person ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用Assignment13跑过的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "medium_config = GPT2Config(n_embd = 768,n_layer = 12, n_head = 12)\n",
    "model = GPT2LMHeadModel(medium_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../lecture13/dials_model.pth')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dials:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.dials = []\n",
    "        self.model = model\n",
    "    \n",
    "    def get_next_word(self, inputs):\n",
    "        tokens_tensor = torch.tensor([tokenizer.encode(inputs)])\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "        \n",
    "        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "        predicted_word = tokenizer.decode([predicted_index])\n",
    "    \n",
    "        return predicted_word\n",
    "    \n",
    "    def generate_answer(self, inputs, max_num_sentences):\n",
    "        answer = ''\n",
    "        num = 0\n",
    "        while True:\n",
    "            next_word = self.get_next_word(inputs)\n",
    "            if next_word == '<|endoftext|>':\n",
    "                if answer:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                answer += next_word\n",
    "                inputs += next_word\n",
    "                if answer[-1] in ['.', '?', '!'] and num > 2:\n",
    "                    break\n",
    "                else:\n",
    "                    num += 1\n",
    "        return answer.strip()\n",
    "    \n",
    "    def start_dial(self, max_num_sentences=1):\n",
    "        print(\"Hi, I'm Robot Alex.\")\n",
    "        print(\"Before the chat begins, you need to know following rules:\")\n",
    "        print(\"If you want to quit the chat, please input: [Quit]\")\n",
    "        print(\"If you want to clear the chat, please input: [Clear]\")\n",
    "        print(\"If you want to recall your last input, please input: [Back]\")\n",
    "        print(\"Let's chat now!\")\n",
    "        \n",
    "        while True:\n",
    "            question = input('Q: ').strip()\n",
    "            if not question:\n",
    "                print('Please enter something!')\n",
    "            if question == '[Quit]':\n",
    "                self.dials = []\n",
    "                print('Chat end.')\n",
    "                break\n",
    "            elif question == '[Clear]':\n",
    "                print('Chat history is gone.')\n",
    "                print()\n",
    "                self.dials = []\n",
    "                continue\n",
    "            elif question == '[Back]':\n",
    "                if len(self.dials) > 0:\n",
    "                    self.dials.pop(-1)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Back to where? We don't have any chat history!\")\n",
    "                    continue\n",
    "            inputs = ' '.join([d+' <|endoftext|> ' for dial in self.dials for d in dial] + \n",
    "                              [question + ' <|endoftext|> '])\n",
    "            answer = self.generate_answer(inputs, max_num_sentences)\n",
    "            self.dials.append([question, answer])\n",
    "            print('A: {}'.format(answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dials = Dials(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Robot Alex.\n",
      "Before the chat begins, you need to know following rules:\n",
      "If you want to quit the chat, please input: [Quit]\n",
      "If you want to clear the chat, please input: [Clear]\n",
      "If you want to recall your last input, please input: [Back]\n",
      "Let's chat now!\n",
      "Q: Hi, I'm back!\n",
      "A: OK.Thanks for asking me.\n",
      "Q: I'm going to improve your performance!\n",
      "A: Oh,,!\n",
      "Q: Great! Isn't it?!\n",
      "A: I'll be right.\n",
      "Q: See you then.\n",
      "A: Good-bye.\n",
      "Q: [Quit]\n",
      "Chat end.\n"
     ]
    }
   ],
   "source": [
    "dials.start_dial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top_k&top_p](https://zhuanlan.zhihu.com/p/80211911)实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单独实现\n",
    "def top_k(distribution, k):\n",
    "    return torch.topk(distribution, k, sorted=Ture)\n",
    "\n",
    "def top_p(sorted_values, p):\n",
    "    cum_prob = torch.cumsum(F.softmax(sorted_values, dim=-1), dim=-1)\n",
    "    for i in range(len(cum_prob)):\n",
    "        if cum_prob[i] >= p:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k top_p 整合\n",
    "def top_k_top_p_filter(distribution, k, p):\n",
    "    values, indices = torch.topk(distribution, k, sorted=True)\n",
    "    \n",
    "    cum_prob = torch.cumsum(F.softmax(values, dim=-1), dim=-1)\n",
    "    \n",
    "    for i in range(len(cum_prob)):\n",
    "        if cum_prob[i] >= p:\n",
    "            return values[:i+1], indices[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编入到生成语句中\n",
    "def generate(input_encode, score, k, p):\n",
    "    tokens_tensor = torch.tensor([input_encode])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        distribution = outputs[0][0, -1, :]\n",
    "        \n",
    "        values, indices = torch.topk(distribution, k, sorted=True)\n",
    "        prob = F.softmax(values, dim=-1)\n",
    "        cum_prob = torch.cumsum(prob, dim=-1)\n",
    "        \n",
    "        for i in range(len(cum_prob)):\n",
    "            if cum_prob[i] >= p:\n",
    "                logprob = torch.log(prob)\n",
    "                scored_sentences = [(input_encode+[indices[j].item()], score+logprob[j].item()) for j in range(i+1)]\n",
    "                return scored_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([40, 1101, 1016, 284, 262], 21.79204750061035),\n",
       " ([40, 1101, 1016, 284, 467], 21.452220916748047),\n",
       " ([40, 1101, 1016, 284, 1011], 21.314449310302734),\n",
       " ([40, 1101, 1016, 284, 307], 20.67784309387207),\n",
       " ([40, 1101, 1016, 284, 616], 20.27678680419922),\n",
       " ([40, 1101, 1016, 284, 326], 20.246797561645508),\n",
       " ([40, 1101, 1016, 284, 1281], 20.23945426940918),\n",
       " ([40, 1101, 1016, 284, 1309], 19.87432098388672),\n",
       " ([40, 1101, 1016, 284, 910], 19.784082412719727),\n",
       " ([40, 1101, 1016, 284, 2822], 19.41615104675293),\n",
       " ([40, 1101, 1016, 284, 423], 19.239120483398438),\n",
       " ([40, 1101, 1016, 284, 869], 18.920625686645508),\n",
       " ([40, 1101, 1016, 284, 731], 18.86473846435547),\n",
       " ([40, 1101, 1016, 284, 257], 18.7321834564209),\n",
       " ([40, 1101, 1016, 284, 2051], 18.695947647094727)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(tokenizer.encode(\"I'm going to\"), 0, 30, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beam search实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(beam_size, sentence, generate, topk, topp, alpha=1):\n",
    "    beam = [(tokenizer.encode(sentence), 0)]\n",
    "    best_sentences = []\n",
    "    punish = lambda x: ((5+x)**alpha)/((5+1)**alpha)  # x=length of sentence\n",
    "    sp = len(beam[0][0]) # start point of new sentence\n",
    "    while beam_size > 0:\n",
    "        candidates = []\n",
    "        for sentence_code, score in beam:\n",
    "            candidates += generate(sentence_code, score, topk, topp)\n",
    "        beam_candi = sorted(candidates, key=lambda x: x[1]/punish(len(x[0][sp:])), reverse=True)[:beam_size]\n",
    "        beam = []\n",
    "        for sentence_code, score in beam_candi:\n",
    "            if sentence_code[-1] == 50256: # tokenizer.encode('<|endoftext|>'):\n",
    "                best_sentences.append((sentence_code[sp:-1], score/punish(len(sentence_code[sp:]))))\n",
    "                beam_size -= 1\n",
    "            else:\n",
    "                beam.append((sentence_code, score))\n",
    "    return best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes. -2.400179802440107\n",
      "You can buy anything. -2.0109613740986045\n",
      "You can buy anything online. -1.467979036475299\n",
      "You can buy it for yourself. -1.6939414302603557\n",
      "You can buy it for free. -2.3183807042928843\n"
     ]
    }
   ],
   "source": [
    "best_sentences = beam_search(5, \"Does money buy happiness? <|endoftext|>\", generate, 20, 0.9)\n",
    "for sentence_code, score in best_sentences:\n",
    "    print(tokenizer.decode(sentence_code), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机取一个回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_random_sentence(sentences_and_scores):\n",
    "    r = random.random()\n",
    "    scores = [score for _, score in sentences_and_scores]\n",
    "    prob = F.softmax(torch.tensor(scores), dim=-1)\n",
    "    cum_prob = torch.cumsum(prob, dim=-1)\n",
    "    for i in range(len(cum_prob)):\n",
    "        if r < cum_prob[i]:\n",
    "            print(cum_prob)\n",
    "            print(r)\n",
    "            return sentences_and_scores[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1230, 0.3046, 0.6172, 0.8665, 1.0000])\n",
      "0.5348381351002666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can buy anything online.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(get_a_random_sentence(best_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合到Dial类中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dials_v1:\n",
    "    \n",
    "    def __init__(self, model, beam_size=5, alpha=1, k=40, p=0.9):\n",
    "        self.dials = []\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        \n",
    "    \n",
    "    def generate(self, input_encode, score):\n",
    "        tokens_tensor = torch.tensor([input_encode])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor)\n",
    "            distribution = outputs[0][0, -1, :]\n",
    "        \n",
    "            values, indices = torch.topk(distribution, self.k, sorted=True)\n",
    "            prob = F.softmax(values, dim=-1)\n",
    "            cum_prob = torch.cumsum(prob, dim=-1)\n",
    "        \n",
    "            for i in range(len(cum_prob)):\n",
    "                if cum_prob[i] >= self.p:\n",
    "                    logprob = torch.log(prob)\n",
    "                    scored_sentences = [(input_encode+[indices[j].item()], score+logprob[j].item()) for j in range(i+1)]\n",
    "                    return scored_sentences\n",
    "                \n",
    "    def beam_search(self, beam_size, inputs, alpha):\n",
    "        beam = [(tokenizer.encode(inputs), 0)]\n",
    "        best_sentences = []\n",
    "        punish = lambda x: ((5+x)**alpha)/((5+1)**alpha)  # x=length of sentence\n",
    "        sp = len(beam[0][0]) # start point of new sentence\n",
    "        while beam_size > 0:\n",
    "            candidates = []\n",
    "            for sentence_code, score in beam:\n",
    "                candidates += self.generate(sentence_code, score)\n",
    "            beam_candi = sorted(candidates, key=lambda x: x[1]/punish(len(x[0][sp:])), reverse=True)[:beam_size]\n",
    "            beam = []\n",
    "            for sentence_code, score in beam_candi:\n",
    "                if sentence_code[-1] == 50256: # tokenizer.encode('<|endoftext|>'):\n",
    "                    best_sentences.append((sentence_code[sp:-1], score/punish(len(sentence_code[sp:]))))\n",
    "                    beam_size -= 1\n",
    "                else:\n",
    "                    beam.append((sentence_code, score))\n",
    "        \n",
    "        r = random.random()\n",
    "        scores = [score for _, score in best_sentences]\n",
    "        prob = F.softmax(torch.tensor(scores), dim=-1)\n",
    "        cum_prob = torch.cumsum(prob, dim=-1)\n",
    "        for i in range(len(cum_prob)):\n",
    "            if r < cum_prob[i]:\n",
    "                return tokenizer.decode(best_sentences[i][0])\n",
    "    \n",
    "    def start_dial(self, showtime=True):\n",
    "        print(\"Hi, I'm Robot Alex.\")\n",
    "        print(\"Before the chat begins, you need to know following rules:\")\n",
    "        print(\"If you want to quit the chat, please input: [Quit]\")\n",
    "        print(\"If you want to clear the chat, please input: [Clear]\")\n",
    "        print(\"If you want to recall your last input, please input: [Back]\")\n",
    "        print(\"Let's chat now!\")\n",
    "        \n",
    "        while True:\n",
    "            question = input('Q: ').strip()\n",
    "            if not question:\n",
    "                print('Please enter something!')\n",
    "            if question == '[Quit]':\n",
    "                self.dials = []\n",
    "                print('Chat end.')\n",
    "                break\n",
    "            elif question == '[Clear]':\n",
    "                print('Chat history is gone.')\n",
    "                print()\n",
    "                self.dials = []\n",
    "                continue\n",
    "            elif question == '[Back]':\n",
    "                if len(self.dials) > 0:\n",
    "                    self.dials.pop(-1)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Back to where? We don't have any chat history!\")\n",
    "                    continue\n",
    "            \n",
    "            inputs = ' '.join([d+' <|endoftext|> ' for dial in self.dials for d in dial] + \n",
    "                              [question + ' <|endoftext|> '])\n",
    "            \n",
    "            start = time.time()\n",
    "            answer = self.beam_search(self.beam_size, inputs, self.alpha)\n",
    "            end = time.time()\n",
    "            time_cost = round(end - start, 1)\n",
    "            \n",
    "            self.dials.append([question, answer])\n",
    "            if showtime:\n",
    "                print('A: {}[{}s]'.format(answer, time_cost))\n",
    "            else:\n",
    "                print('A: {}'.format(answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "dial = Dials_v1(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Robot Alex.\n",
      "Before the chat begins, you need to know following rules:\n",
      "If you want to quit the chat, please input: [Quit]\n",
      "If you want to clear the chat, please input: [Clear]\n",
      "If you want to recall your last input, please input: [Back]\n",
      "Let's chat now!\n",
      "Q: Hello!\n",
      "A: Hello?[1.7s]\n",
      "Q: I feel really good today!\n",
      "A: Me, too. But it's just a day's work.[4.8s]\n",
      "Q: What's your job?\n",
      "A: Not too bad.[4.1s]\n",
      "Q: A banker?\n",
      "A: I got this job by a job in the summer.[9.7s]\n",
      "Q: What do you do in your job?\n",
      "A: I got it.[10.2s]\n",
      "Q: [Clear]\n",
      "Chat history is gone.\n",
      "\n",
      "Q: I want to work as a coder.\n",
      "A: That sounds good.[2.8s]\n",
      "Q: But l have learnt accounting for six years.\n",
      "A: Well, that ’ s ok.[4.9s]\n",
      "Q: Thank you for supporting me.\n",
      "A: It ’ s my pleasure.[6.3s]\n",
      "Q: [Clear]\n",
      "Chat history is gone.\n",
      "\n",
      "Q: What is the fastest way to earn money?\n",
      "A: You have to pay a fine.[4.3s]\n",
      "Q: Illegal way?\n",
      "A: Here, pay is your driver's license.[4.7s]\n",
      "Q: [Clear]\n",
      "Chat history is gone.\n",
      "\n",
      "Q: What is the best way to earn money?\n",
      "A: You should ask me.[4.2s]\n",
      "Q: Yes! Please tell me!\n",
      "A:  this is your only work.[4.0s]\n",
      "Q: Then?\n",
      "A: Ok, here you are.[4.7s]\n",
      "Q: Yes, do you know how to earn a lot of money?\n",
      "A: [4.8s]\n",
      "Q: ?\n",
      "A: OK, here you are.[7.5s]\n",
      "Q: [Clear]\n",
      "Chat history is gone.\n",
      "\n",
      "Q: [Quit]\n",
      "Chat end.\n"
     ]
    }
   ],
   "source": [
    "dial.start_dial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">服务器上跑语料代码不知道为什么断了2次，结果现在还没跑完，用的是上周的5000个对话的语料，可能效果并没有太大的提高。<br>\n",
    "而且随着对话次数变多，计算速度也变慢了。<br>\n",
    "网上有资料说可以通过past来提高计算速度，类似于缓存，等研究好了加上去～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">这次主要添加了topktopp筛选以及随机选取回复，效果感觉不错。而且不需要像上次那样通过判断标点来直接截断长度来防止句子无意义过长。<br>\n",
    "等提高速度后尝试一下中文对话～"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
